{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dee30d9-c85c-47ee-bbb0-0076e5c86951",
   "metadata": {},
   "source": [
    "# 循环神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67ce0cc-bac2-4f12-a9bd-f1c17655cfab",
   "metadata": {},
   "source": [
    "\n",
    "\r\n",
    "### 1. 无隐状态的神经网络\r\n",
    "无隐状态的神经网络是指那些在处理输入时**不保留任何历史信息**的网络。这类网络通常是前馈神经网络（Feedforward Neural Networks），比如多层感知机（MLP）或卷积神经网络（CNN）。它们的输入和输出之间没有时间上的依赖关。\r\n",
    "\r\n",
    "#### 1.1 数学表示\r\n",
    "假设输入为 $\\mathbf{x}_t$，输出为 $\\mathbf{y}_t$，无隐状态的神经网络可以表示为：\r\n",
    "$$\r\n",
    "\\mathbf{y}_t = f(\\mathbf{x}_t; \\mathbf{W})\r\n",
    "$$\r\n",
    "其中：\r\n",
    "- $f$ 是一个非线性函数（如ReLU、Sigmoid等），\r\n",
    "- $\\mathbf{W}$ 是网的参数（权重和偏置）。\r\n",
    "\r\n",
    "#### 1.2 工作机制\r\n",
    "- 每个输入 $\\mathbf{x}_t$ 是独立的，网络不会记住之前处理过的输入。\r\n",
    "- 输出 $\\mathbf{y}_t$ 仅依赖于当前输入 $\\mathbf{x}_t$，与之前的输入 $\\mathbf{x}_{t-1}, \\mathbf{x}_{-2}, \\dots$ 无关。\r\n",
    "\r\n",
    "#### 1.3 优点\r\n",
    "- 简单且易于实现。\r\n",
    "- 适合处理静数据（如图像分类、静态特征提取等）。\r\n",
    "\r\n",
    "#### 1.4 缺点\r\n",
    "- 无法处理序列数据中的时间依赖性。\r\n",
    "- 对于时间列、文本、语音等数据，无法捕捉上下文信息。\r\n",
    "\r\n",
    "#### 1.5 适用场景\r\n",
    "- 图像分类（CNN）。\n",
    "- 静态数据的回归或分类任务（MLP）。\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 2. 有隐状态的循环神经网络（RNN）\r\n",
    "有隐状态的循环神经网络（RNN）通过引入**隐状态（Hidden State）**来捕捉序列数据中的时间依赖性。隐状态可以被看作是一个“记忆单元，它保存了之前时间步的信息，并在处理当前输入时使用这些信息。\r\n",
    "\r\n",
    "#### 2.1 数学表示\r\n",
    "假设在时间步 $t$：\r\n",
    "- 输入为 $\\mathbf{x}_t$，\r\n",
    "- 隐状态为 $\\mathbf{h}_t$，\r\n",
    "- 输出为 $\\mathbf{y}_t$。\r\n",
    "\r\n",
    "RNN的更新规则可以表示为：\r\n",
    "$$\r\n",
    "\\mathbf{h}_t = f(\\mathbf{x}_t, \\mathbf{h}_{t-1}; \\mathbf{W})\r\n",
    "$$\r\n",
    "其中：\r\n",
    "- $f$ 是一个非线性函数（如tanh、ReLU等），\r\n",
    "- $\\mathbf{W}$ 是网络的参数（权重和偏置），\r\n",
    "- $\\mathbf{h}_{t-1}$ 是上一个时间步的隐状态。\r\n",
    "\r\n",
    "输出 $\\mathbf{y}_t$ 通常由隐状态 $\\mathbf{h}_t$ 决定：\r\n",
    "$$\r\n",
    "\\mathbf{y}_t = g(\\mathbf{h}_t; \\mathbf{V})\r\n",
    "$$\r\n",
    "其中：\r",
    "- $g$ 是输出函数（如softmax、线性变换等），\r\n",
    "- $\\mathbf{V}$ 是输出层的参数。\r\n",
    "\r\n",
    "#### 2.2 工作机制\r\n",
    "- RNN通过隐状态 $\\mathbf{h}_t$ 保留了之前时间步的信息。\r\n",
    "- 在每个时间步 $t$，RNN接收当前输入 $\\mathbf{x}_t$ 和上一个隐状态 $\\mathbf{h}_{t-1}$，计算当前隐状态 $\\mathbf{h}_t$ 和输出 $\\mathbf{y}_t$。\r\n",
    "- 隐状态 $\\mathbf{h}_t$ 可以被看作是对过去有输入 $\\mathbf{x}_1, \\mathbf{x}_2, \\dots, \\mathbf{x}_t$ 的总结。\r\n",
    "\r\n",
    "#### 2.3 优点\n",
    "- 能够处理序列数据中的时间依赖性。\r\n",
    "- 适合处理变长序列（如文本、语音、时间序列等）。\r\n",
    "- 通过隐状态捕捉上下文信息。\r\n",
    "\r\n",
    "#### 2.4 缺点\r\n",
    "- 训练过程中容易\n",
    "出现梯消失或梯度爆炸问题。\r\n",
    "- 长期依赖问题：RNN难以捕捉远距离时间步之间的依赖关系（尽管LSTM和GRU等改进模型可以缓解这一问题）。\r\n",
    "\r\n",
    "#### .5 适用场景\r\n",
    "- 自然语言处理（如文本生成、机器翻译）。\r\n",
    "- 时间序列预测（如股票价格预测、天气预测）。\r\n",
    "- 语音识别和处理。\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 3. 无隐状态与有隐状态网络的对比\r\n",
    "\r\n",
    "| 特性                  | 无隐状态网络（MLP/CNN）          | 有隐状态网络（RNN）               |\r\n",
    "|-----------------------|----------------------------------|----------------------------------|\r\n",
    "| **输入输出关系**       | 每个输入独立处理                 | 输入和输出之间存在时间依赖性     |\r\n",
    "| **隐状态**            | 无隐状态                         | 有隐状态，保存历史信息           |\r\n",
    "| **时间依赖性**        | 无法捕捉时间依赖性               | 能够捕捉时间依赖性               |\r\n",
    "| **适用数据**          | 静态数据（如图像）               | 序列数据（如文本、语音、时间序列）|\r\n",
    "| **训练难度**          | 相对简单                         | 较复杂，能出现梯度消失/爆炸    |\r\n",
    " **典型应用**          | 图像分类、静态特征提取           | 机器翻译、语音识别、时间序列预测 |\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 4. 数学上的进一步分析\r\n",
    "\r\n",
    "#### 4.1 无隐状态网络的局限性\r\n",
    "无隐状态网络的输出仅依赖于当前输入：\r\n",
    "$$\r",
    "\\mathbf{y}_t = f(\\mathbf{x}_t; \\mathbf{W})\r\n",
    "$$\r\n",
    "这种形式无法建模序列数据中的时间依赖性。例如，在自然语言处理中，一个词的含义可能依赖于前面的词。\r\n",
    "\r\n",
    "#### 4.2 RNN的时间展开\r\n",
    "RNN可以通过时间展开（Unrolling）来表示。假设序列长度为 $T$，RNN可以展开为：\r\n",
    "$$\r\n",
    "\\mathbf{h}_t = f(\\mathbf{x}_t, \\mathbf{h}_{t-1}; \\mathbf{W}), \\quad t = 1, 2, \\dots, T\r\n",
    "$$\r\n",
    "输出为：\r",
    "$$\r\n",
    "\\mathbf{y}_t = g(\\mathbf{h}_t; \\mathbf{V}), \\quad t = 1, 2, \\dots, T\r\n",
    "$$\r\n",
    "通过这种展开，RNN可以显式地建模序列数据中\n",
    "的解决RNN的局限性，后续提出了LSTM（长短期记忆网络）和GRU（门控循环单元），它们通过引入门控机制更好地处理长期依赖问题。\r\n",
    "\r\n",
    "希望这个更详细的解释能帮助你更好地理解无隐状态网络和有隐状态循环神经网络的区别与联系！如果还有疑问，欢迎继续提问！解决RNN的局限性，后续提出了LSTM（长短期记忆网络）和GRU（门控循环单元），它们通过引入门控机制更好地处理长期依赖问题。\r\n",
    "\r\n",
    "希望这个更详细的解释能帮助你更好地理解无隐状态网络和有隐状态循环神经网络的区别与联系！如果还有疑问，欢迎继续提问！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08672047-8f91-4043-8845-07c3b6d3db39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c535c97b-5fd6-4869-9d1d-29c7e881dac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9035,  0.3974, -0.4007, -1.7206],\n",
       "        [-4.2657, -2.9477, -2.1996, -0.8024],\n",
       "        [-2.4657,  0.3214,  1.7296, -3.0419]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, W_xh = torch.normal(0, 1, (3, 1)), torch.normal(0, 1, (1, 4))\n",
    "H, W_hh = torch.normal(0, 1, (3, 4)), torch.normal(0, 1, (4, 4))\n",
    "torch.matmul(X, W_xh) + torch.matmul(H,W_hh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2a9abd-775c-414a-969f-c19b4ee301fb",
   "metadata": {},
   "source": [
    "首先，我们定义矩阵X、W_xh、H和W_hh，它们的形状分别为(3,1)、(1,4)、(3,4)和(4,4)。分别将X乘以W_xh，将H乘以W_hh，然后将\n",
    "这两个乘法相加，我们得到一个形状为(3,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe14583-1116-416a-b8db-c7fc014839ac",
   "metadata": {},
   "source": [
    "现在，我们沿列（轴1）拼接矩阵X和H，沿行（轴0）拼接矩阵W_xh和W_hh。这两个拼接分别产生形状(3, 5)和\n",
    "形状(5, 4)的矩阵。再将这两个拼接的矩阵相乘，我们得到与上面相同形状(3, 4)的输出矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "334d2735-cf1a-49dd-a6d4-0064bbbb767d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9035,  0.3974, -0.4007, -1.7206],\n",
       "        [-4.2657, -2.9477, -2.1996, -0.8024],\n",
       "        [-2.4657,  0.3214,  1.7296, -3.0419]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(torch.cat((X, H), 1), torch.cat((W_xh, W_hh),0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a411cc2b-12c4-4a87-8a80-a8069fe4f5cc",
   "metadata": {},
   "source": [
    "## 基于循环神经网络的字符级语言模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43532dd-2488-453c-8176-209151a91922",
   "metadata": {},
   "source": [
    "基于循环神经网络（RNN）的**字符级语言模型**是一种用于生成或预测文本序列的模型。它的目标是根据前面的字符序列，预测下一个字符的概率分布。以下是其核心思想和数学表达：\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **问题定义**\n",
    "字符级语言模型的输入是一个字符序列 $ x_1, x_2, \\dots, x_t $，其中每个 $ x_i $ 是一个字符（通常用 one-hot 向量表示）。模型的目标是预测下一个字符 $ x_{t+1} $ 的概率分布。\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **RNN 的结构**\n",
    "RNN 通过隐状态 $ h_t $ 来捕捉序列中的依赖关系。其计算过程如下：\n",
    "\n",
    "1. **输入表示**：\n",
    "   - 每个字符 $ x_t $ 是一个 one-hot 向量，维度为词汇表大小 $ V $。\n",
    "   - 通过嵌入矩阵 $ E $ 将 one-hot 向量映射为稠密向量：\n",
    "     $$\n",
    "     e_t = E x_t\n",
    "     $$\n",
    "     其中 $ e_t \\in \\mathbb{R}^d $，$ d $ 是嵌入维度。\n",
    "\n",
    "2. **隐状态更新**：\n",
    "   - RNN 的隐状态 $ h_t $ 通过当前输入 $ e_t $ 和前一个隐状态 $ h_{t-1} $ 计算得到：\n",
    "     $$\n",
    "     h_t = \\tanh(W_h h_{t-1} + W_e e_t + b_h)\n",
    "     $$\n",
    "     其中：\n",
    "     - $ W_h $ 是隐状态到隐状态的权重矩阵。\n",
    "     - $ W_e $ 是输入到隐状态的权重矩阵。\n",
    "     - $ b_h $ 是偏置项。\n",
    "     - $ \\tanh $ 是激活函数。\n",
    "\n",
    "3. **输出概率分布**：\n",
    "   - 通过隐状态 $ h_t $ 计算下一个字符的概率分布：\n",
    "     $$\n",
    "     o_t = W_o h_t + b_o\n",
    "     $$\n",
    "     其中：\n",
    "     - $ W_o $ 是隐状态到输出的权重矩阵。\n",
    "     - $ b_o $ 是偏置项。\n",
    "   - 使用 softmax 函数将 $ o_t $ 转换为概率分布：\n",
    "     $$\n",
    "     P(x_{t+1} | x_1, x_2, \\dots, x_t) = \\text{softmax}(o_t)\n",
    "     $$\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **训练目标**\n",
    "模型的训练目标是最大化训练数据的对数似然，即最小化负对数似然损失（交叉熵损失）。对于序列 $ x_1, x_2, \\dots, x_T $，损失函数为：\n",
    "$$\n",
    "\\mathcal{L} = -\\sum_{t=1}^{T-1} \\log P(x_{t+1} | x_1, x_2, \\dots, x_t)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **生成文本**\n",
    "训练完成后，模型可以通过以下方式生成文本：\n",
    "1. 给定一个初始字符 $ x_1 $。\n",
    "2. 根据当前字符和隐状态，预测下一个字符的概率分布。\n",
    "3. 从概率分布中采样一个字符作为 $ x_{t+1} $。\n",
    "4. 将 $ x_{t+1} $ 作为输入，重复上述过程，直到生成所需长度的文本。\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **关键特点**\n",
    "- **字符级建模**：模型以字符为单位进行预测，适合生成任意文本（包括拼写错误、标点符号等）。\n",
    "- **序列依赖**：RNN 的隐状态 $ h_t $ 捕捉了序列中的长期依赖关系。\n",
    "- **灵活性**：可以处理任意长度的序列，但可能面临梯度消失或梯度爆炸问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a2a7c7-c9cc-4f99-844a-fc13dc06a04b",
   "metadata": {},
   "source": [
    "## 困惑度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc05be9f-d571-4393-a320-3041027ef143",
   "metadata": {},
   "source": [
    "困惑度（Perplexity）是评估语言模型性能的常用指标，主要用于衡量模型对测试数据的预测能力。以下是其关键点：\n",
    "\n",
    "### 定义\n",
    "困惑度是衡量语言模型在给定测试数据上预测下一个词的不确定性。困惑度越低，模型的预测能力越强。\n",
    "\n",
    "### 计算公式\n",
    "困惑度公式为：\n",
    "$$\n",
    "\\text{Perplexity} = 2^{-\\frac{1}{N} \\sum_{i=1}^{N} \\log_2 P(w_i | w_1, w_2, \\dots, w_{i-1})} \n",
    "$$\n",
    "其中：\n",
    "- $ N $ 是测试数据的总词数。\n",
    "- $ P(w_i | w_1, w_2, \\dots, w_{i-1}) $ 是模型预测当前词 $ w_i $ 的概率。\n",
    "\n",
    "### 解释\n",
    "- **低困惑度**：模型对数据预测更准确。\n",
    "- **高困惑度**：模型预测能力较差。\n",
    "\n",
    "### 应用\n",
    "1. **模型比较**：用于比较不同语言模型的性能。\n",
    "2. **调参**：帮助调整模型超参数。\n",
    "3. **过拟合检测**：训练集困惑度低而测试集困惑度高可能表明过拟合。\n",
    "\n",
    "### 示例\n",
    "假设测试数据有100个词，模型预测每个词的概率为0.01，则困惑度为：\n",
    "$$\n",
    "\\text{Perplexity} = 2^{-\\frac{1}{100} \\times 100 \\times \\log_2 0.01} = 100 \n",
    "$$\n",
    "\n",
    "### 优缺点\n",
    "**优点**：\n",
    "- 计算简单，易于理解。\n",
    "- 广泛应用于语言模型评估。\n",
    "\n",
    "**缺点**：\n",
    "- 无法捕捉语义和上下文连贯性。\n",
    "- 对数据分布敏感，可能无法全面反映模型性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78045e4-b206-4dd2-b882-73a1e91eb01f",
   "metadata": {},
   "source": [
    "# 循环神经网络的从零开始实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6affab1-1789-40fa-9455-a9069f74e247",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f012c258-b559-463b-8886-94f77a15a8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, num_steps = 32, 35\n",
    "train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d83aaaf-0fa0-4771-b342-03596163004b",
   "metadata": {},
   "source": [
    "## 独热编码"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78d1fda-471a-4734-9b5f-c96a69e0e4a5",
   "metadata": {},
   "source": [
    "回想一下，在train_iter中，每个词元都表示为一个数字索引，将这些索引直接输入神经网络可能会使学习变\n",
    "得困难。我们通常将每个词元表示为更具表现力的特征向量。最简单的表示称为独热编码（one‐hotencoding）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35871c83-47b3-4e91-894a-82575a3602e1",
   "metadata": {},
   "source": [
    "简言之，将每个索引映射为相互不同的单位向量：假设词表中不同词元的数目为N（即len(vocab)），词元索\n",
    "引的范围为0到N−1。如果词元的索引是整数i，那么我们将创建一个长度为N的全0向量，并将第i处的元素\n",
    "设置为1。此向量是原始词元的一个独热向量。索引为0和2的独热向量如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb3c51e4-8392-4ec1-bddb-09da150cb3a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.one_hot(torch.tensor([0, 2]), len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1c1c61-0b44-474d-9af4-02379c0ab1f2",
   "metadata": {},
   "source": [
    "我们每次采样的小批量数据形状是二维张量：（批量大小，时间步数）。one_hot函数将这样一个小批量数据转\n",
    "换成三维张量，张量的最后一个维度等于词表大小（len(vocab)）。我们经常转换输入的维度，以便获得形状\n",
    "为（时间步数，批量大小，词表大小）的输出。这将使我们能够更方便地通过最外层的维度，一步一步地更\n",
    "新小批量数据的隐状态。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20977ff7-aa59-4e8f-8c74-81e3d2f7aa9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2, 28])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(10).reshape((2, 5))\n",
    "F.one_hot(X.T, 28).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976af399-753f-4a39-a7cc-f3342bf72059",
   "metadata": {},
   "source": [
    "## 初始化模型参数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3c73ff-e045-49aa-8ca5-c233b1225451",
   "metadata": {},
   "source": [
    "隐藏单元数num_hiddens是一个可调的超参数。当训练语\n",
    "言模型时，输入和输出来自相同的词表。因此，它们具有相同的维度，即词表的大小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b13ecfdc-423a-48a4-a86a-b57b8f982333",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  # 导入PyTorch库\n",
    "\n",
    "def get_params(vocab_size, num_hiddens, device):\n",
    "    \"\"\"\n",
    "    初始化RNN模型的参数。\n",
    "\n",
    "    参数:\n",
    "    - vocab_size: 词汇表的大小，即输入和输出的维度。\n",
    "    - num_hiddens: 隐藏层的大小，即隐藏层的神经元数量。\n",
    "    - device: 计算设备（如CPU或GPU），用于指定参数存储的位置。\n",
    "\n",
    "    返回:\n",
    "    - params: 包含所有模型参数的列表，这些参数已经设置为需要计算梯度。\n",
    "    \"\"\"\n",
    "    \n",
    "    # 输入和输出的维度相同，因为这是一个简单的RNN模型\n",
    "    num_inputs = num_outputs = vocab_size\n",
    "\n",
    "    # 定义一个辅助函数，用于生成服从正态分布的随机数\n",
    "    def normal(shape):\n",
    "        \"\"\"\n",
    "        生成服从正态分布的随机数，并乘以0.01以缩小初始值的范围。\n",
    "\n",
    "        参数:\n",
    "        - shape: 张量的形状。\n",
    "\n",
    "        返回:\n",
    "        - 一个形状为`shape`的张量，其值服从正态分布，且乘以0.01。\n",
    "        \"\"\"\n",
    "        return torch.randn(size=shape, device=device) * 0.01\n",
    "\n",
    "    # 初始化隐藏层的参数\n",
    "    W_xh = normal((num_inputs, num_hiddens))  # 输入到隐藏层的权重矩阵\n",
    "    W_hh = normal((num_hiddens, num_hiddens))  # 隐藏层到隐藏层的权重矩阵\n",
    "    b_h = torch.zeros(num_hiddens, device=device)  # 隐藏层的偏置向量\n",
    "\n",
    "    # 初始化输出层的参数\n",
    "    W_hq = normal((num_hiddens, num_outputs))  # 隐藏层到输出层的权重矩阵\n",
    "    b_q = torch.zeros(num_outputs, device=device)  # 输出层的偏置向量\n",
    "\n",
    "    # 将所有参数放入一个列表中\n",
    "    params = [W_xh, W_hh, b_h, W_hq, b_q]\n",
    "\n",
    "    # 设置所有参数为需要计算梯度\n",
    "    for param in params:\n",
    "        param.requires_grad_(True)\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397776f3-0874-4e66-990a-73763d7f2c2d",
   "metadata": {},
   "source": [
    "## 循环神经网络模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f61b2b0-d357-459e-bef2-74382756edcc",
   "metadata": {},
   "source": [
    "为了定义循环神经网络模型，我们首先需要一个init_rnn_state函数在初始化时返回隐状态。这个函数的返\n",
    "回是一个张量，张量全用0填充，形状为（批量大小，隐藏单元数）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46f890b9-96e5-46b6-a93f-adf2ea9d74a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  # 导入PyTorch库\n",
    "\n",
    "def init_rnn_state(batch_size, num_hiddens, device):\n",
    "    \"\"\"\n",
    "    初始化RNN的隐藏状态。\n",
    "\n",
    "    参数:\n",
    "    - batch_size: 批次大小，即一次输入的数据样本数量。\n",
    "    - num_hiddens: 隐藏层的大小，即隐藏层的神经元数量。\n",
    "    - device: 计算设备（如CPU或GPU），用于指定隐藏状态存储的位置。\n",
    "\n",
    "    返回:\n",
    "    - 一个包含隐藏状态的元组，隐藏状态是一个全零的张量。\n",
    "    \"\"\"\n",
    "    # 初始化隐藏状态为一个全零的张量\n",
    "    # 形状为 (batch_size, num_hiddens)，表示每个样本的隐藏状态\n",
    "    return (torch.zeros((batch_size, num_hiddens), device=device), )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6f73be-53ce-4842-a707-ffe8d7797dbf",
   "metadata": {},
   "source": [
    "再考虑定义一个RNN函数，定义如何在一个时间步内计算隐状态和输出。循环神经网络模型通过inputs最外层的维度\n",
    "实现循环，以便逐时间步更新小批量数据的隐状态H。此外，这里使用tanh函数作为激活函数。\n",
    "当元素在实数上满足均匀分布时，tanh函数的平均值为0。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a93a9f9-96c9-4ec3-adfb-ade08a6ac973",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  # 导入PyTorch库\n",
    "\n",
    "def rnn(inputs, state, params):\n",
    "    \"\"\"\n",
    "    实现RNN的前向传播。\n",
    "\n",
    "    参数:\n",
    "    - inputs: 输入数据，形状为 (时间步数量, 批量大小, 词表大小)。\n",
    "    - state: 初始隐藏状态，是一个元组，包含一个形状为 (批量大小, 隐藏层大小) 的张量。\n",
    "    - params: 包含RNN模型参数的列表，顺序为 [W_xh, W_hh, b_h, W_hq, b_q]。\n",
    "\n",
    "    返回:\n",
    "    - outputs: 所有时间步的输出，形状为 (时间步数量 * 批量大小, 词表大小)。\n",
    "    - (H,): 更新后的隐藏状态，是一个元组，包含一个形状为 (批量大小, 隐藏层大小) 的张量。\n",
    "    \"\"\"\n",
    "    # 解包参数\n",
    "    W_xh, W_hh, b_h, W_hq, b_q = params\n",
    "\n",
    "    # 解包隐藏状态\n",
    "    H, = state\n",
    "\n",
    "    # 用于存储每个时间步的输出\n",
    "    outputs = []\n",
    "\n",
    "    # 遍历每个时间步的输入\n",
    "    # X的形状：(批量大小, 词表大小)\n",
    "    for X in inputs:\n",
    "        # 更新隐藏状态 H\n",
    "        # torch.mm(X, W_xh): 计算输入到隐藏层的线性变换\n",
    "        # torch.mm(H, W_hh): 计算隐藏层到隐藏层的线性变换\n",
    "        # b_h: 隐藏层的偏置\n",
    "        # torch.tanh: 应用tanh激活函数\n",
    "        H = torch.tanh(torch.mm(X, W_xh) + torch.mm(H, W_hh) + b_h)\n",
    "\n",
    "        # 计算当前时间步的输出 Y\n",
    "        # torch.mm(H, W_hq): 计算隐藏层到输出层的线性变换\n",
    "        # b_q: 输出层的偏置\n",
    "        Y = torch.mm(H, W_hq) + b_q\n",
    "\n",
    "        # 将当前时间步的输出 Y 添加到 outputs 列表中\n",
    "        outputs.append(Y)\n",
    "\n",
    "    # 将所有时间步的输出拼接成一个张量\n",
    "    # outputs 的形状：(时间步数量 * 批量大小, 词表大小)\n",
    "    outputs = torch.cat(outputs, dim=0)\n",
    "\n",
    "    # 返回输出和更新后的隐藏状态\n",
    "    return outputs, (H,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad95903f-73b6-46a8-9497-8df1f8854290",
   "metadata": {},
   "source": [
    "定义了所有需要的函数之后，接下来我们创建一个类来包装这些函数，并存储从零开始实现的循环神经网络\n",
    "模型的参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31c8f9d9-c0ab-434f-9cda-a3ed854a80cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModelScratch:  # @save\n",
    "    \"\"\"从零开始实现的循环神经网络模型\"\"\"\n",
    "    def __init__(self, vocab_size, num_hiddens, device, get_params, init_state, forward_fn):\n",
    "        \"\"\"\n",
    "        初始化RNN模型。\n",
    "\n",
    "        参数:\n",
    "        - vocab_size: 词汇表的大小。\n",
    "        - num_hiddens: 隐藏层的大小（隐藏单元的数量）。\n",
    "        - device: 计算设备（如CPU或GPU）。\n",
    "        - get_params: 用于初始化模型参数的函数。\n",
    "        - init_state: 用于初始化隐藏状态的函数。\n",
    "        - forward_fn: 用于实现前向传播的函数。\n",
    "        \"\"\"\n",
    "        self.vocab_size, self.num_hiddens = vocab_size, num_hiddens\n",
    "        self.params = get_params(vocab_size, num_hiddens, device)  # 初始化模型参数\n",
    "        self.init_state, self.forward_fn = init_state, forward_fn  # 初始化隐藏状态和前向传播函数\n",
    "\n",
    "    def __call__(self, X, state):\n",
    "        \"\"\"\n",
    "        定义模型的前向传播。\n",
    "\n",
    "        参数:\n",
    "        - X: 输入数据，形状为 (批量大小, 时间步数量)。\n",
    "        - state: 隐藏状态，形状为 (批量大小, 隐藏层大小)。\n",
    "\n",
    "        返回:\n",
    "        - 输出和新的隐藏状态。\n",
    "        \"\"\"\n",
    "        # 将输入X转换为one-hot编码，形状为 (时间步数量, 批量大小, 词表大小)\n",
    "        X = F.one_hot(X.T, self.vocab_size).type(torch.float32)\n",
    "        # 调用前向传播函数\n",
    "        return self.forward_fn(X, state, self.params)\n",
    "\n",
    "    def begin_state(self, batch_size, device):\n",
    "        \"\"\"\n",
    "        初始化隐藏状态。\n",
    "\n",
    "        参数:\n",
    "        - batch_size: 批量大小。\n",
    "        - device: 计算设备（如CPU或GPU）。\n",
    "\n",
    "        返回:\n",
    "        - 初始隐藏状态。\n",
    "        \"\"\"\n",
    "        return self.init_state(batch_size, self.num_hiddens, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162d9d90-f8c5-4c0a-a029-ba8a7ec5ddde",
   "metadata": {},
   "source": [
    "再进行一个模型实例化与前向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f4e238e-af4b-4b47-96f6-4578d08c0165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 28]), 1, torch.Size([2, 512]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 设置隐藏层大小\n",
    "num_hiddens = 512\n",
    "\n",
    "# 实例化RNN模型\n",
    "net = RNNModelScratch(len(vocab), num_hiddens, d2l.try_gpu(), get_params, init_rnn_state, rnn)\n",
    "\n",
    "# 初始化隐藏状态\n",
    "state = net.begin_state(X.shape[0], d2l.try_gpu())\n",
    "\n",
    "# 前向传播\n",
    "Y, new_state = net(X.to(d2l.try_gpu()), state)\n",
    "\n",
    "# 输出结果的形状\n",
    "Y.shape, len(new_state), new_state[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c04486-2e61-483b-a654-502d7ddd52d6",
   "metadata": {},
   "source": [
    "我们可以看到输出形状是（时间步数×批量大小，词表大小），而隐状态形状保持不变，即（批量大小，隐藏\n",
    "单元数）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a55bbb-67a4-4067-b3ee-3db8f2efc328",
   "metadata": {},
   "source": [
    "## 预测"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ea651a-e651-4fc6-b1cf-53d24a14adc1",
   "metadata": {},
   "source": [
    "先定义预测函数来生成prefix之后的新字符，其中的prefix是一个用户提供的包含多个字符的字符\n",
    "串。在循环遍历prefix中的开始字符时，我们不断地将隐状态传递到下一个时间步，但是不生成任何输出。这\n",
    "被称为预热（warm‐up）期，因为在此期间模型会自我更新（例如，更新隐状态），但不会进行预测。预热期\n",
    "结束后，隐状态的值通常比刚开始的初始值更适合预测，从而预测字符并输出它们。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0057c607-e2f8-46d2-b7ba-8549e9e6a2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ch8(prefix, num_preds, net, vocab, device): #@save\n",
    "    \"\"\"在prefix后面生成新字符\"\"\"\n",
    "    # 初始化RNN的隐藏状态\n",
    "    state = net.begin_state(batch_size=1, device=device)\n",
    "    \n",
    "    # 将前缀的第一个字符转换为索引，并存储在outputs列表中\n",
    "    outputs = [vocab[prefix[0]]]\n",
    "    \n",
    "    # 定义一个lambda函数，用于获取当前时间步的输入\n",
    "    get_input = lambda: torch.tensor([outputs[-1]], device=device).reshape((1, 1))\n",
    "    \n",
    "    # 预热期：处理前缀的剩余字符，更新RNN的隐藏状态\n",
    "    for y in prefix[1:]:  # 遍历前缀的剩余字符\n",
    "        _, state = net(get_input(), state)  # 更新隐藏状态\n",
    "        outputs.append(vocab[y])  # 将字符索引添加到outputs列表中\n",
    "    \n",
    "    # 预测num_preds步，生成新字符\n",
    "    for _ in range(num_preds):  # 循环num_preds次\n",
    "        y, state = net(get_input(), state)  # 预测下一个字符并更新隐藏状态\n",
    "        outputs.append(int(y.argmax(dim=1).reshape(1)))  # 将预测结果转换为索引并添加到outputs列表中\n",
    "    \n",
    "    # 将outputs列表中的索引转换为字符，并拼接成最终的字符串返回\n",
    "    return ''.join([vocab.idx_to_token[i] for i in outputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae5c6de-4909-454f-8b62-6bbedce6732b",
   "metadata": {},
   "source": [
    "现在我们可以测试predict_ch8函数。我们将前缀指定为time traveller，并基于这个前缀生成10个后续字\n",
    "符。鉴于我们还没有训练网络，它会生成荒谬的预测结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb2f6db1-a502-4538-97ca-179caa616409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'time traveller gfxnjshbhb'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_ch8('time traveller ', 10, net, vocab, d2l.try_gpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46c4426-aabc-42dc-8a59-31699e1f4676",
   "metadata": {},
   "source": [
    "## 梯度裁剪"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ac43f8-b4ca-4aea-9308-303005e64844",
   "metadata": {},
   "source": [
    "**梯度裁剪（Gradient Clipping）**是一种在训练深度学习模型时常用的技术，主要用于解决梯度爆炸（Gradient Explosion）问题。梯度爆炸是指在训练过程中，梯度值变得非常大，导致模型参数更新幅度过大，从而破坏模型的稳定性，甚至导致训练失败。\n",
    "\n",
    "梯度裁剪通过限制梯度的大小，确保梯度值在一个合理的范围内，从而稳定训练过程。\n",
    "\n",
    "---\n",
    "\n",
    "### 梯度爆炸的原因\n",
    "在深度神经网络中，尤其是循环神经网络（RNN）或长序列模型中，梯度是通过反向传播算法计算的。由于反向传播是通过链式法则逐层计算梯度的，如果网络层数较深或某些层的权重较大，梯度可能会在反向传播过程中不断累积，导致梯度值变得非常大。\n",
    "\n",
    "梯度爆炸的表现：\n",
    "- 模型参数更新幅度过大。\n",
    "- 损失函数值突然变成 `NaN`（非数值）。\n",
    "- 训练过程不稳定。\n",
    "\n",
    "---\n",
    "\n",
    "### 梯度裁剪的原理\n",
    "梯度裁剪的核心思想是对梯度进行限制，使其不超过某个阈值。具体来说，梯度裁剪有两种常见的方式：\n",
    "\n",
    "1. **按值裁剪（Value Clipping）**：\n",
    "   - 直接对梯度中的每个元素进行裁剪，如果梯度的绝对值超过某个阈值，则将其截断到该阈值。\n",
    "   - 公式：\n",
    "     $$\n",
    "     g_{i} = \\begin{cases}\n",
    "     \\text{threshold} & \\text{if } g_{i} > \\text{threshold} \\\\\n",
    "     -\\text{threshold} & \\text{if } g_{i} < -\\text{threshold} \\\\\n",
    "     g_{i} & \\text{otherwise}\n",
    "     \\end{cases}\n",
    "     $$\n",
    "   - 其中，$ g_{i} $ 是梯度的第 $ i $ 个元素。\n",
    "\n",
    "2. **按范数裁剪（Norm Clipping）**：\n",
    "   - 计算梯度的范数（通常是 L2 范数），如果梯度的范数超过某个阈值，则将梯度按比例缩放，使其范数等于阈值。\n",
    "   - 公式：\n",
    "     $$\n",
    "     g = \\begin{cases}\n",
    "     g \\cdot \\frac{\\text{threshold}}{\\|g\\|} & \\text{if } \\|g\\| > \\text{threshold} \\\\\n",
    "     g & \\text{otherwise}\n",
    "     \\end{cases}\n",
    "     $$\n",
    "   - 其中，$ \\|g\\| $ 是梯度的 L2 范数。\n",
    "\n",
    "---\n",
    "\n",
    "### 梯度裁剪的作用\n",
    "1. **防止梯度爆炸**：\n",
    "   - 通过限制梯度的大小，避免模型参数更新幅度过大，从而稳定训练过程。\n",
    "\n",
    "2. **加速收敛**：\n",
    "   - 梯度裁剪可以使训练过程更加平滑，避免因梯度爆炸导致的震荡，从而加速模型的收敛。\n",
    "\n",
    "3. **提高模型稳定性**：\n",
    "   - 在训练深度模型（如 RNN、Transformer）时，梯度裁剪是一种常用的正则化技术，可以提高模型的稳定性。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0fae5532-13b6-4f82-87bd-ee72644334ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_clipping(net, theta): #@save\n",
    "    \"\"\"裁剪梯度\"\"\"\n",
    "    # 判断模型类型并获取可训练参数\n",
    "    if isinstance(net, nn.Module):\n",
    "        params = [p for p in net.parameters() if p.requires_grad]\n",
    "    else:\n",
    "        params = net.params\n",
    "    \n",
    "    # 计算梯度的L2范数\n",
    "    norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))\n",
    "    \n",
    "    # 如果梯度的范数超过阈值，则进行裁剪\n",
    "    if norm > theta:\n",
    "        for param in params:\n",
    "            param.grad[:] *= theta / norm  # 裁剪梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f79982-8547-4102-a249-090d1b3af3fe",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1a4488-f4af-456d-ab93-a01fb66b2677",
   "metadata": {},
   "source": [
    "在训练模型之前，让定义一个函数在一个迭代周期内训练模型。这里的三个特点是：\n",
    "\n",
    "1. 序列数据的不同采样方法（随机采样和顺序分区）将导致隐状态初始化的差异。\n",
    "2. 我们在更新模型参数之前裁剪梯度。这样的操作的目的是，即使训练过程中某个点上发生了梯度爆炸，也能保证模型不会发散。\n",
    "3. 我们用困惑度来评价模型。这样的度量确保了不同长度的序列具有可比性。\n",
    "\n",
    "具体来说，当使用顺序分区时，我们只在每个迭代周期的开始位置初始化隐状态。由于下一个小批量数据中\n",
    "的第i个子序列样本与当前第i个子序列样本相邻，因此当前小批量数据最后一个样本的隐状态，将用于初\r\n",
    "化下一个小批量数据第一个样本的隐状态。这样，存储在隐状态中的序列的历史信息可以在一个迭代周内\r\n",
    "流经相邻的子序列。然而，在任何一点隐状态的计算，都依赖于同一迭代周期中前面所有的小批量数，这\r\n",
    "使得梯度计算变得复杂。为了降低计算量，在处理任何一个小批量数据之前，我们先分离梯度，使隐状态\r\n",
    "的梯度计算总是限制在一个小批量数据的\n",
    "\n",
    "当使用随机抽样时，因为每个样本都是在一个随机位置抽样的，因此需要为每个迭代周期重新初始化隐状\n",
    "态。与3.6节中的train_epoch_ch3函数相同，updater是更新模型参数的常用函数。它既可以是从头开始\r\n",
    "现的d2l.sgd函数，也可以是深度学习框架中内置的优化函数。时间步内。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "22b2063b-7688-47e3-aedc-446fc83edef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_ch8(net, train_iter, loss, updater, device, use_random_iter):\n",
    "    \"\"\"训练网络一个迭代周期（定义见第8章）\"\"\"\n",
    "    state, timer = None, d2l.Timer()  # 初始化隐藏状态和计时器\n",
    "    metric = d2l.Accumulator(2)  # 累积训练损失和词元数量\n",
    "\n",
    "    # 遍历训练数据\n",
    "    for X, Y in train_iter:\n",
    "        # 初始化隐藏状态\n",
    "        if state is None or use_random_iter:\n",
    "            state = net.begin_state(batch_size=X.shape[0], device=device)\n",
    "        else:\n",
    "            if isinstance(net, nn.Module) and not isinstance(state, tuple):\n",
    "                state.detach_()  # 对于nn.GRU，state是单个张量\n",
    "            else:\n",
    "                for s in state:\n",
    "                    s.detach_()  # 对于nn.LSTM或自定义模型，state是元组\n",
    "\n",
    "        # 准备数据\n",
    "        y = Y.T.reshape(-1)\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # 前向传播\n",
    "        y_hat, state = net(X, state)\n",
    "\n",
    "        # 计算损失\n",
    "        l = loss(y_hat, y.long()).mean()\n",
    "\n",
    "        # 反向传播和梯度裁剪\n",
    "        if isinstance(updater, torch.optim.Optimizer):\n",
    "            updater.zero_grad()\n",
    "            l.backward()\n",
    "            grad_clipping(net, 1)  # 裁剪梯度\n",
    "            updater.step()\n",
    "        else:\n",
    "            l.backward()\n",
    "            grad_clipping(net, 1)  # 裁剪梯度\n",
    "            updater(batch_size=1)  # 手动更新参数\n",
    "\n",
    "        # 累积损失和词元数量\n",
    "        metric.add(l * y.numel(), y.numel())\n",
    "\n",
    "    # 返回平均损失和每个词元的训练时间\n",
    "    return math.exp(metric[0] / metric[1]), metric[1] / timer.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcf6e62-0fea-4412-bcf4-52e003c185a1",
   "metadata": {},
   "source": [
    "循环神经网络模型的训练函数既支持从零开始实现，也可以使用高级API来实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c8751498-d3e5-40eb-847d-30ddb912301c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ch8(net, train_iter, vocab, lr, num_epochs, device, use_random_iter=False):\n",
    "    \"\"\"训练模型（定义见第8章）\"\"\"\n",
    "    loss = nn.CrossEntropyLoss()  # 初始化交叉熵损失函数\n",
    "    animator = d2l.Animator(xlabel='epoch', ylabel='perplexity', legend=['train'], xlim=[10, num_epochs])  # 初始化动画工具\n",
    "\n",
    "    # 初始化优化器\n",
    "    if isinstance(net, nn.Module):\n",
    "        updater = torch.optim.SGD(net.parameters(), lr)  # 使用 PyTorch 的 SGD 优化器\n",
    "    else:\n",
    "        updater = lambda batch_size: d2l.sgd(net.params, lr, batch_size)  # 使用自定义的梯度下降函数\n",
    "\n",
    "    # 定义预测函数\n",
    "    predict = lambda prefix: predict_ch8(prefix, 50, net, vocab, device)  # 定义预测函数\n",
    "\n",
    "    # 训练和预测\n",
    "    for epoch in range(num_epochs):\n",
    "        ppl, speed = train_epoch_ch8(net, train_iter, loss, updater, device, use_random_iter)  # 训练一个周期\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(predict('time traveller'))  # 每隔 10 个周期生成预测文本\n",
    "            animator.add(epoch + 1, [ppl])  # 将当前周期的困惑度添加到动画工具中\n",
    "\n",
    "    # 打印最终结果\n",
    "    print(f'困惑度 {ppl:.1f}, {speed:.1f} 词元/秒 {str(device)}')\n",
    "    print(predict('time traveller'))  # 生成最终的预测文本\n",
    "    print(predict('traveller'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c60a21-f70a-4509-9dd6-fb59aab52a92",
   "metadata": {},
   "source": [
    "现在，我们训练循环神经网络模型。因为我们在数据集中只使用了10000个词元，所以模型需要更多的迭代周期来更好地收敛。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e02bfd1a-03a2-4605-86c3-bf117e5adf26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "困惑度 1.1, 26209.4 词元/秒 cuda:0\n",
      "time travelleryou can show black is white by argument said filby\n",
      "travelleryou can show black is white by argument said filby\n"
     ]
    }
   ],
   "source": [
    "num_epochs, lr = 500, 1\n",
    "train_ch8(net, train_iter, vocab, lr, num_epochs, d2l.try_gpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ab0319-f76e-4a8d-8164-74f281d67122",
   "metadata": {},
   "source": [
    "最后在检查一下使用随机抽样方法的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "82202e08-2e3c-44ad-a060-2a2c4444bbdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "困惑度 1.3, 36488.3 词元/秒 cuda:0\n",
      "time traveller proceeded anyreal body must have extension in fou\n",
      "traveller proceeded anyreal body must have extension in fou\n"
     ]
    }
   ],
   "source": [
    "net = RNNModelScratch(len(vocab), num_hiddens, d2l.try_gpu(), get_params,\n",
    "                     init_rnn_state, rnn)\n",
    "train_ch8(net, train_iter, vocab, lr, num_epochs, d2l.try_gpu(),\n",
    "             use_random_iter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054eac50-19e5-4a6a-ab0f-7ac56bfc8afb",
   "metadata": {},
   "source": [
    "# 循环神经网络的简洁实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aaccddc-53f8-4dec-80b7-a2945af65580",
   "metadata": {},
   "source": [
    "同样实现的是对该数据集的操作，不过这里使用的是深度学习框架中的高级API实现操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2a5aa15-1135-4690-84f0-0f11cae2cd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from d2l import torch as d2l\n",
    "batch_size, num_steps = 32, 35\n",
    "train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447ba9a0-4012-4b6b-8474-d03dbd86afd2",
   "metadata": {},
   "source": [
    "## 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d4e6960-59e0-48b7-bc00-297cc4ad9381",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hiddens = 256\n",
    "rnn_layer = nn.RNN(len(vocab), num_hiddens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e452d2a-dd90-4009-984f-5ac240acadfa",
   "metadata": {},
   "source": [
    "使用张量来初始化隐状态，它的形状是（隐藏层数，批量大小，隐藏单元数）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "784b543d-f509-4688-9446-4fa1024a7b2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 256])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = torch.zeros((1, batch_size, num_hiddens))\n",
    "state.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b51dc06-5a73-437f-b69b-3247ff9b810a",
   "metadata": {},
   "source": [
    "通过一个隐状态和一个输入，我们就可以用更新后的隐状态计算输出。需要强调的是，rnn_layer的“输出”（Y）不涉及输出层的计算：它是指每个时间步的隐状态，这些隐状态可以用作后续输出层的输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55413deb-98a4-4b1d-bddc-b981018b3d19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([35, 32, 256]), torch.Size([1, 32, 256]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.rand(size=(num_steps, batch_size, len(vocab)))\n",
    "Y, state_new = rnn_layer(X, state)\n",
    "Y.shape, state_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66680680-c14e-4563-8c23-a42beb45c387",
   "metadata": {},
   "source": [
    "为一个完整的循环神经网络模型定义了一个RNNModel类。注意，rnn_layer只包含隐藏的循环层，我们还需要创建一个单独的输出层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "099ba689-0bbd-4730-b620-699c81f7f5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    \"\"\"循环神经网络模型\"\"\"\n",
    "    def __init__(self, rnn_layer, vocab_size, **kwargs):\n",
    "        super(RNNModel, self).__init__(**kwargs)\n",
    "        self.rnn = rnn_layer  # RNN 层\n",
    "        self.vocab_size = vocab_size  # 词汇表大小\n",
    "        self.num_hiddens = self.rnn.hidden_size  # 隐藏单元数\n",
    "\n",
    "        # 检查 RNN 是否双向\n",
    "        if not self.rnn.bidirectional:\n",
    "            self.num_directions = 1  # 单向\n",
    "            self.linear = nn.Linear(self.num_hiddens, self.vocab_size)  # 全连接层\n",
    "        else:\n",
    "            self.num_directions = 2  # 双向\n",
    "            self.linear = nn.Linear(self.num_hiddens * 2, self.vocab_size)  # 全连接层\n",
    "\n",
    "    def forward(self, inputs, state):\n",
    "        # 将输入的词元索引转换为 one-hot 编码\n",
    "        X = F.one_hot(inputs.T.long(), self.vocab_size)\n",
    "        X = X.to(torch.float32)  # 转换为浮点型\n",
    "\n",
    "        # 将输入和初始状态传递给 RNN 层\n",
    "        Y, state = self.rnn(X, state)\n",
    "\n",
    "        # 将 RNN 的输出重塑为 (num_steps * batch_size, hidden_size)\n",
    "        output = self.linear(Y.reshape((-1, Y.shape[-1])))\n",
    "\n",
    "        return output, state\n",
    "\n",
    "    def begin_state(self, device, batch_size=1):\n",
    "        # 初始化隐藏状态\n",
    "        if not isinstance(self.rnn, nn.LSTM):\n",
    "            # nn.GRU 以张量作为隐状态\n",
    "            return torch.zeros((self.num_directions * self.rnn.num_layers,\n",
    "                                batch_size, self.num_hiddens),\n",
    "                               device=device)\n",
    "        else:\n",
    "            # nn.LSTM 以元组作为隐状态\n",
    "            return (torch.zeros((self.num_directions * self.rnn.num_layers,\n",
    "                                 batch_size, self.num_hiddens), device=device),\n",
    "                    torch.zeros((self.num_directions * self.rnn.num_layers,\n",
    "                                 batch_size, self.num_hiddens), device=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fb651a-6d50-4b9a-9801-8ba4c05ae38e",
   "metadata": {},
   "source": [
    "## 训练与预测"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31bf164-1519-4457-8b04-268b3a0d2c49",
   "metadata": {},
   "source": [
    "在训练模型之前，让我们基于一个具有随机权重的模型进行预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "347bd11b-33c4-44ee-82af-98b685b6971c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'time travellerrrcttttttt'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = d2l.try_gpu()\n",
    "net = RNNModel(rnn_layer, vocab_size=len(vocab))\n",
    "net = net.to(device)\n",
    "d2l.predict_ch8('time traveller', 10, net, vocab, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c1c125-1aab-4824-9ce8-c20e8e992fa8",
   "metadata": {},
   "source": [
    "很明显，这种模型根本不能输出好的结果。接下来，我们使用之前定义的超参数调用train_ch8，并且使\n",
    "用高级API训练模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffd8cc4-184d-4a21-a19f-db8d4e40a896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time traveller het oncad filby if elargertmestersame time and ha\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"268.55pt\" height=\"183.35625pt\" viewBox=\"0 0 268.55 183.35625\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2025-01-31T23:09:58.014343</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.9.2, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M -0 183.35625 \n",
       "L 268.55 183.35625 \n",
       "L 268.55 0 \n",
       "L -0 0 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 56.50625 145.8 \n",
       "L 251.80625 145.8 \n",
       "L 251.80625 7.2 \n",
       "L 56.50625 7.2 \n",
       "z\n",
       "\" style=\"fill: #ffffff\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <path d=\"M 92.377679 145.8 \n",
       "L 92.377679 7.2 \n",
       "\" clip-path=\"url(#pdf8aa21f36)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_2\">\n",
       "      <defs>\n",
       "       <path id=\"m6d6412146f\" d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m6d6412146f\" x=\"92.377679\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 100 -->\n",
       "      <g transform=\"translate(82.833929 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-31\" d=\"M 794 531 \n",
       "L 1825 531 \n",
       "L 1825 4091 \n",
       "L 703 3866 \n",
       "L 703 4441 \n",
       "L 1819 4666 \n",
       "L 2450 4666 \n",
       "L 2450 531 \n",
       "L 3481 531 \n",
       "L 3481 0 \n",
       "L 794 0 \n",
       "L 794 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \n",
       "Q 1547 4250 1301 3770 \n",
       "Q 1056 3291 1056 2328 \n",
       "Q 1056 1369 1301 889 \n",
       "Q 1547 409 2034 409 \n",
       "Q 2525 409 2770 889 \n",
       "Q 3016 1369 3016 2328 \n",
       "Q 3016 3291 2770 3770 \n",
       "Q 2525 4250 2034 4250 \n",
       "z\n",
       "M 2034 4750 \n",
       "Q 2819 4750 3233 4129 \n",
       "Q 3647 3509 3647 2328 \n",
       "Q 3647 1150 3233 529 \n",
       "Q 2819 -91 2034 -91 \n",
       "Q 1250 -91 836 529 \n",
       "Q 422 1150 422 2328 \n",
       "Q 422 3509 836 4129 \n",
       "Q 1250 4750 2034 4750 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <path d=\"M 132.234821 145.8 \n",
       "L 132.234821 7.2 \n",
       "\" clip-path=\"url(#pdf8aa21f36)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m6d6412146f\" x=\"132.234821\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 200 -->\n",
       "      <g transform=\"translate(122.691071 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \n",
       "L 3431 531 \n",
       "L 3431 0 \n",
       "L 469 0 \n",
       "L 469 531 \n",
       "Q 828 903 1448 1529 \n",
       "Q 2069 2156 2228 2338 \n",
       "Q 2531 2678 2651 2914 \n",
       "Q 2772 3150 2772 3378 \n",
       "Q 2772 3750 2511 3984 \n",
       "Q 2250 4219 1831 4219 \n",
       "Q 1534 4219 1204 4116 \n",
       "Q 875 4013 500 3803 \n",
       "L 500 4441 \n",
       "Q 881 4594 1212 4672 \n",
       "Q 1544 4750 1819 4750 \n",
       "Q 2544 4750 2975 4387 \n",
       "Q 3406 4025 3406 3419 \n",
       "Q 3406 3131 3298 2873 \n",
       "Q 3191 2616 2906 2266 \n",
       "Q 2828 2175 2409 1742 \n",
       "Q 1991 1309 1228 531 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-32\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <path d=\"M 172.091964 145.8 \n",
       "L 172.091964 7.2 \n",
       "\" clip-path=\"url(#pdf8aa21f36)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m6d6412146f\" x=\"172.091964\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 300 -->\n",
       "      <g transform=\"translate(162.548214 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-33\" d=\"M 2597 2516 \n",
       "Q 3050 2419 3304 2112 \n",
       "Q 3559 1806 3559 1356 \n",
       "Q 3559 666 3084 287 \n",
       "Q 2609 -91 1734 -91 \n",
       "Q 1441 -91 1130 -33 \n",
       "Q 819 25 488 141 \n",
       "L 488 750 \n",
       "Q 750 597 1062 519 \n",
       "Q 1375 441 1716 441 \n",
       "Q 2309 441 2620 675 \n",
       "Q 2931 909 2931 1356 \n",
       "Q 2931 1769 2642 2001 \n",
       "Q 2353 2234 1838 2234 \n",
       "L 1294 2234 \n",
       "L 1294 2753 \n",
       "L 1863 2753 \n",
       "Q 2328 2753 2575 2939 \n",
       "Q 2822 3125 2822 3475 \n",
       "Q 2822 3834 2567 4026 \n",
       "Q 2313 4219 1838 4219 \n",
       "Q 1578 4219 1281 4162 \n",
       "Q 984 4106 628 3988 \n",
       "L 628 4550 \n",
       "Q 988 4650 1302 4700 \n",
       "Q 1616 4750 1894 4750 \n",
       "Q 2613 4750 3031 4423 \n",
       "Q 3450 4097 3450 3541 \n",
       "Q 3450 3153 3228 2886 \n",
       "Q 3006 2619 2597 2516 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-33\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <path d=\"M 211.949107 145.8 \n",
       "L 211.949107 7.2 \n",
       "\" clip-path=\"url(#pdf8aa21f36)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m6d6412146f\" x=\"211.949107\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 400 -->\n",
       "      <g transform=\"translate(202.405357 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-34\" d=\"M 2419 4116 \n",
       "L 825 1625 \n",
       "L 2419 1625 \n",
       "L 2419 4116 \n",
       "z\n",
       "M 2253 4666 \n",
       "L 3047 4666 \n",
       "L 3047 1625 \n",
       "L 3713 1625 \n",
       "L 3713 1100 \n",
       "L 3047 1100 \n",
       "L 3047 0 \n",
       "L 2419 0 \n",
       "L 2419 1100 \n",
       "L 313 1100 \n",
       "L 313 1709 \n",
       "L 2253 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-34\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <path d=\"M 251.80625 145.8 \n",
       "L 251.80625 7.2 \n",
       "\" clip-path=\"url(#pdf8aa21f36)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m6d6412146f\" x=\"251.80625\" y=\"145.8\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 500 -->\n",
       "      <g transform=\"translate(242.2625 160.398438) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \n",
       "L 3169 4666 \n",
       "L 3169 4134 \n",
       "L 1269 4134 \n",
       "L 1269 2991 \n",
       "Q 1406 3038 1543 3061 \n",
       "Q 1681 3084 1819 3084 \n",
       "Q 2600 3084 3056 2656 \n",
       "Q 3513 2228 3513 1497 \n",
       "Q 3513 744 3044 326 \n",
       "Q 2575 -91 1722 -91 \n",
       "Q 1428 -91 1123 -41 \n",
       "Q 819 9 494 109 \n",
       "L 494 744 \n",
       "Q 775 591 1075 516 \n",
       "Q 1375 441 1709 441 \n",
       "Q 2250 441 2565 725 \n",
       "Q 2881 1009 2881 1497 \n",
       "Q 2881 1984 2565 2268 \n",
       "Q 2250 2553 1709 2553 \n",
       "Q 1456 2553 1204 2497 \n",
       "Q 953 2441 691 2322 \n",
       "L 691 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-35\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"127.246094\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_6\">\n",
       "     <!-- epoch -->\n",
       "     <g transform=\"translate(138.928125 174.076563) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-65\" d=\"M 3597 1894 \n",
       "L 3597 1613 \n",
       "L 953 1613 \n",
       "Q 991 1019 1311 708 \n",
       "Q 1631 397 2203 397 \n",
       "Q 2534 397 2845 478 \n",
       "Q 3156 559 3463 722 \n",
       "L 3463 178 \n",
       "Q 3153 47 2828 -22 \n",
       "Q 2503 -91 2169 -91 \n",
       "Q 1331 -91 842 396 \n",
       "Q 353 884 353 1716 \n",
       "Q 353 2575 817 3079 \n",
       "Q 1281 3584 2069 3584 \n",
       "Q 2775 3584 3186 3129 \n",
       "Q 3597 2675 3597 1894 \n",
       "z\n",
       "M 3022 2063 \n",
       "Q 3016 2534 2758 2815 \n",
       "Q 2500 3097 2075 3097 \n",
       "Q 1594 3097 1305 2825 \n",
       "Q 1016 2553 972 2059 \n",
       "L 3022 2063 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-70\" d=\"M 1159 525 \n",
       "L 1159 -1331 \n",
       "L 581 -1331 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2969 \n",
       "Q 1341 3281 1617 3432 \n",
       "Q 1894 3584 2278 3584 \n",
       "Q 2916 3584 3314 3078 \n",
       "Q 3713 2572 3713 1747 \n",
       "Q 3713 922 3314 415 \n",
       "Q 2916 -91 2278 -91 \n",
       "Q 1894 -91 1617 61 \n",
       "Q 1341 213 1159 525 \n",
       "z\n",
       "M 3116 1747 \n",
       "Q 3116 2381 2855 2742 \n",
       "Q 2594 3103 2138 3103 \n",
       "Q 1681 3103 1420 2742 \n",
       "Q 1159 2381 1159 1747 \n",
       "Q 1159 1113 1420 752 \n",
       "Q 1681 391 2138 391 \n",
       "Q 2594 391 2855 752 \n",
       "Q 3116 1113 3116 1747 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6f\" d=\"M 1959 3097 \n",
       "Q 1497 3097 1228 2736 \n",
       "Q 959 2375 959 1747 \n",
       "Q 959 1119 1226 758 \n",
       "Q 1494 397 1959 397 \n",
       "Q 2419 397 2687 759 \n",
       "Q 2956 1122 2956 1747 \n",
       "Q 2956 2369 2687 2733 \n",
       "Q 2419 3097 1959 3097 \n",
       "z\n",
       "M 1959 3584 \n",
       "Q 2709 3584 3137 3096 \n",
       "Q 3566 2609 3566 1747 \n",
       "Q 3566 888 3137 398 \n",
       "Q 2709 -91 1959 -91 \n",
       "Q 1206 -91 779 398 \n",
       "Q 353 888 353 1747 \n",
       "Q 353 2609 779 3096 \n",
       "Q 1206 3584 1959 3584 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-63\" d=\"M 3122 3366 \n",
       "L 3122 2828 \n",
       "Q 2878 2963 2633 3030 \n",
       "Q 2388 3097 2138 3097 \n",
       "Q 1578 3097 1268 2742 \n",
       "Q 959 2388 959 1747 \n",
       "Q 959 1106 1268 751 \n",
       "Q 1578 397 2138 397 \n",
       "Q 2388 397 2633 464 \n",
       "Q 2878 531 3122 666 \n",
       "L 3122 134 \n",
       "Q 2881 22 2623 -34 \n",
       "Q 2366 -91 2075 -91 \n",
       "Q 1284 -91 818 406 \n",
       "Q 353 903 353 1747 \n",
       "Q 353 2603 823 3093 \n",
       "Q 1294 3584 2113 3584 \n",
       "Q 2378 3584 2631 3529 \n",
       "Q 2884 3475 3122 3366 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-68\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 4863 \n",
       "L 1159 4863 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-65\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-70\" x=\"61.523438\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6f\" x=\"125\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-63\" x=\"186.181641\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-68\" x=\"241.162109\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <path d=\"M 56.50625 136.445201 \n",
       "L 251.80625 136.445201 \n",
       "\" clip-path=\"url(#pdf8aa21f36)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_12\">\n",
       "      <defs>\n",
       "       <path id=\"m702223d134\" d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use xlink:href=\"#m702223d134\" x=\"56.50625\" y=\"136.445201\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 1.275 -->\n",
       "      <g transform=\"translate(20.878125 140.24442) scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path id=\"DejaVuSans-2e\" d=\"M 684 794 \n",
       "L 1344 794 \n",
       "L 1344 0 \n",
       "L 684 0 \n",
       "L 684 794 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "        <path id=\"DejaVuSans-37\" d=\"M 525 4666 \n",
       "L 3525 4666 \n",
       "L 3525 4397 \n",
       "L 1831 0 \n",
       "L 1172 0 \n",
       "L 2766 4134 \n",
       "L 525 4134 \n",
       "L 525 4666 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-37\" x=\"159.033203\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"222.65625\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_13\">\n",
       "      <path d=\"M 56.50625 113.345121 \n",
       "L 251.80625 113.345121 \n",
       "\" clip-path=\"url(#pdf8aa21f36)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_14\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m702223d134\" x=\"56.50625\" y=\"113.345121\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 1.300 -->\n",
       "      <g transform=\"translate(20.878125 117.14434) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-33\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"222.65625\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_15\">\n",
       "      <path d=\"M 56.50625 90.245041 \n",
       "L 251.80625 90.245041 \n",
       "\" clip-path=\"url(#pdf8aa21f36)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_16\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m702223d134\" x=\"56.50625\" y=\"90.245041\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 1.325 -->\n",
       "      <g transform=\"translate(20.878125 94.044259) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-33\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-32\" x=\"159.033203\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"222.65625\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_17\">\n",
       "      <path d=\"M 56.50625 67.14496 \n",
       "L 251.80625 67.14496 \n",
       "\" clip-path=\"url(#pdf8aa21f36)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_18\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m702223d134\" x=\"56.50625\" y=\"67.14496\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 1.350 -->\n",
       "      <g transform=\"translate(20.878125 70.944179) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-33\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"159.033203\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"222.65625\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_19\">\n",
       "      <path d=\"M 56.50625 44.04488 \n",
       "L 251.80625 44.04488 \n",
       "\" clip-path=\"url(#pdf8aa21f36)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_20\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m702223d134\" x=\"56.50625\" y=\"44.04488\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 1.375 -->\n",
       "      <g transform=\"translate(20.878125 47.844098) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-33\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-37\" x=\"159.033203\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-35\" x=\"222.65625\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_6\">\n",
       "     <g id=\"line2d_21\">\n",
       "      <path d=\"M 56.50625 20.944799 \n",
       "L 251.80625 20.944799 \n",
       "\" clip-path=\"url(#pdf8aa21f36)\" style=\"fill: none; stroke: #b0b0b0; stroke-width: 0.8; stroke-linecap: square\"/>\n",
       "     </g>\n",
       "     <g id=\"line2d_22\">\n",
       "      <g>\n",
       "       <use xlink:href=\"#m702223d134\" x=\"56.50625\" y=\"20.944799\" style=\"stroke: #000000; stroke-width: 0.8\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_12\">\n",
       "      <!-- 1.400 -->\n",
       "      <g transform=\"translate(20.878125 24.744018) scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-31\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-2e\" x=\"63.623047\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-34\" x=\"95.410156\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"159.033203\"/>\n",
       "       <use xlink:href=\"#DejaVuSans-30\" x=\"222.65625\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"text_13\">\n",
       "     <!-- perplexity -->\n",
       "     <g transform=\"translate(14.798438 101.626563) rotate(-90) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-72\" d=\"M 2631 2963 \n",
       "Q 2534 3019 2420 3045 \n",
       "Q 2306 3072 2169 3072 \n",
       "Q 1681 3072 1420 2755 \n",
       "Q 1159 2438 1159 1844 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1341 3275 1631 3429 \n",
       "Q 1922 3584 2338 3584 \n",
       "Q 2397 3584 2469 3576 \n",
       "Q 2541 3569 2628 3553 \n",
       "L 2631 2963 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6c\" d=\"M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-78\" d=\"M 3513 3500 \n",
       "L 2247 1797 \n",
       "L 3578 0 \n",
       "L 2900 0 \n",
       "L 1881 1375 \n",
       "L 863 0 \n",
       "L 184 0 \n",
       "L 1544 1831 \n",
       "L 300 3500 \n",
       "L 978 3500 \n",
       "L 1906 2253 \n",
       "L 2834 3500 \n",
       "L 3513 3500 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-69\" d=\"M 603 3500 \n",
       "L 1178 3500 \n",
       "L 1178 0 \n",
       "L 603 0 \n",
       "L 603 3500 \n",
       "z\n",
       "M 603 4863 \n",
       "L 1178 4863 \n",
       "L 1178 4134 \n",
       "L 603 4134 \n",
       "L 603 4863 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-74\" d=\"M 1172 4494 \n",
       "L 1172 3500 \n",
       "L 2356 3500 \n",
       "L 2356 3053 \n",
       "L 1172 3053 \n",
       "L 1172 1153 \n",
       "Q 1172 725 1289 603 \n",
       "Q 1406 481 1766 481 \n",
       "L 2356 481 \n",
       "L 2356 0 \n",
       "L 1766 0 \n",
       "Q 1100 0 847 248 \n",
       "Q 594 497 594 1153 \n",
       "L 594 3053 \n",
       "L 172 3053 \n",
       "L 172 3500 \n",
       "L 594 3500 \n",
       "L 594 4494 \n",
       "L 1172 4494 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-79\" d=\"M 2059 -325 \n",
       "Q 1816 -950 1584 -1140 \n",
       "Q 1353 -1331 966 -1331 \n",
       "L 506 -1331 \n",
       "L 506 -850 \n",
       "L 844 -850 \n",
       "Q 1081 -850 1212 -737 \n",
       "Q 1344 -625 1503 -206 \n",
       "L 1606 56 \n",
       "L 191 3500 \n",
       "L 800 3500 \n",
       "L 1894 763 \n",
       "L 2988 3500 \n",
       "L 3597 3500 \n",
       "L 2059 -325 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-70\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"63.476562\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"125\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-70\" x=\"166.113281\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6c\" x=\"229.589844\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-65\" x=\"257.373047\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-78\" x=\"317.146484\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"376.326172\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-74\" x=\"404.109375\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-79\" x=\"443.318359\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_23\">\n",
       "    <path d=\"M 56.50625 100.502263 \n",
       "L 60.491964 103.03963 \n",
       "L 64.477679 77.312628 \n",
       "L 68.463393 115.985413 \n",
       "L 72.449107 112.32489 \n",
       "L 76.434821 97.862632 \n",
       "L 80.420536 112.271861 \n",
       "L 84.40625 109.122198 \n",
       "L 88.391964 50.74732 \n",
       "L 92.377679 132.806907 \n",
       "L 96.363393 105.062083 \n",
       "L 100.349107 58.194339 \n",
       "L 104.334821 73.90756 \n",
       "L 108.320536 95.141731 \n",
       "L 112.30625 108.735348 \n",
       "L 116.291964 118.247673 \n",
       "L 120.277679 92.200451 \n",
       "L 124.263393 30.337139 \n",
       "L 128.249107 73.824115 \n",
       "L 132.234821 98.435723 \n",
       "L 136.220536 77.129727 \n",
       "L 140.20625 95.848681 \n",
       "L 144.191964 130.363496 \n",
       "L 148.177679 32.930733 \n",
       "L 152.163393 13.5 \n",
       "L 156.149107 47.386183 \n",
       "L 160.134821 88.253895 \n",
       "L 164.120536 113.794161 \n",
       "L 168.10625 128.153039 \n",
       "L 172.091964 87.38706 \n",
       "L 176.077679 84.945384 \n",
       "L 180.063393 70.096373 \n",
       "L 184.049107 139.5 \n",
       "L 188.034821 53.638812 \n",
       "L 192.020536 66.940212 \n",
       "L 196.00625 108.986063 \n",
       "L 199.991964 48.2652 \n",
       "L 203.977679 63.652645 \n",
       "L 207.963393 99.987168 \n",
       "L 211.949107 74.119486 \n",
       "L 215.934821 49.362466 \n",
       "L 219.920536 86.112211 \n",
       "L 223.90625 78.020643 \n",
       "L 227.891964 101.793112 \n",
       "L 231.877679 93.855765 \n",
       "L 235.863393 78.962527 \n",
       "L 239.849107 26.351041 \n",
       "L 243.834821 93.575027 \n",
       "L 247.820536 57.213779 \n",
       "\" clip-path=\"url(#pdf8aa21f36)\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 56.50625 145.8 \n",
       "L 56.50625 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 251.80625 145.8 \n",
       "L 251.80625 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 56.50625 145.8 \n",
       "L 251.80625 145.8 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 56.50625 7.2 \n",
       "L 251.80625 7.2 \n",
       "\" style=\"fill: none; stroke: #000000; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n",
       "   </g>\n",
       "   <g id=\"legend_1\">\n",
       "    <g id=\"patch_7\">\n",
       "     <path d=\"M 63.50625 29.878125 \n",
       "L 118.78125 29.878125 \n",
       "Q 120.78125 29.878125 120.78125 27.878125 \n",
       "L 120.78125 14.2 \n",
       "Q 120.78125 12.2 118.78125 12.2 \n",
       "L 63.50625 12.2 \n",
       "Q 61.50625 12.2 61.50625 14.2 \n",
       "L 61.50625 27.878125 \n",
       "Q 61.50625 29.878125 63.50625 29.878125 \n",
       "z\n",
       "\" style=\"fill: #ffffff; opacity: 0.8; stroke: #cccccc; stroke-linejoin: miter\"/>\n",
       "    </g>\n",
       "    <g id=\"line2d_24\">\n",
       "     <path d=\"M 65.50625 20.298438 \n",
       "L 75.50625 20.298438 \n",
       "L 85.50625 20.298438 \n",
       "\" style=\"fill: none; stroke: #1f77b4; stroke-width: 1.5; stroke-linecap: square\"/>\n",
       "    </g>\n",
       "    <g id=\"text_14\">\n",
       "     <!-- train -->\n",
       "     <g transform=\"translate(93.50625 23.798438) scale(0.1 -0.1)\">\n",
       "      <defs>\n",
       "       <path id=\"DejaVuSans-61\" d=\"M 2194 1759 \n",
       "Q 1497 1759 1228 1600 \n",
       "Q 959 1441 959 1056 \n",
       "Q 959 750 1161 570 \n",
       "Q 1363 391 1709 391 \n",
       "Q 2188 391 2477 730 \n",
       "Q 2766 1069 2766 1631 \n",
       "L 2766 1759 \n",
       "L 2194 1759 \n",
       "z\n",
       "M 3341 1997 \n",
       "L 3341 0 \n",
       "L 2766 0 \n",
       "L 2766 531 \n",
       "Q 2569 213 2275 61 \n",
       "Q 1981 -91 1556 -91 \n",
       "Q 1019 -91 701 211 \n",
       "Q 384 513 384 1019 \n",
       "Q 384 1609 779 1909 \n",
       "Q 1175 2209 1959 2209 \n",
       "L 2766 2209 \n",
       "L 2766 2266 \n",
       "Q 2766 2663 2505 2880 \n",
       "Q 2244 3097 1772 3097 \n",
       "Q 1472 3097 1187 3025 \n",
       "Q 903 2953 641 2809 \n",
       "L 641 3341 \n",
       "Q 956 3463 1253 3523 \n",
       "Q 1550 3584 1831 3584 \n",
       "Q 2591 3584 2966 3190 \n",
       "Q 3341 2797 3341 1997 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "       <path id=\"DejaVuSans-6e\" d=\"M 3513 2113 \n",
       "L 3513 0 \n",
       "L 2938 0 \n",
       "L 2938 2094 \n",
       "Q 2938 2591 2744 2837 \n",
       "Q 2550 3084 2163 3084 \n",
       "Q 1697 3084 1428 2787 \n",
       "Q 1159 2491 1159 1978 \n",
       "L 1159 0 \n",
       "L 581 0 \n",
       "L 581 3500 \n",
       "L 1159 3500 \n",
       "L 1159 2956 \n",
       "Q 1366 3272 1645 3428 \n",
       "Q 1925 3584 2291 3584 \n",
       "Q 2894 3584 3203 3211 \n",
       "Q 3513 2838 3513 2113 \n",
       "z\n",
       "\" transform=\"scale(0.015625)\"/>\n",
       "      </defs>\n",
       "      <use xlink:href=\"#DejaVuSans-74\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-72\" x=\"39.208984\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-61\" x=\"80.322266\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-69\" x=\"141.601562\"/>\n",
       "      <use xlink:href=\"#DejaVuSans-6e\" x=\"169.384766\"/>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"pdf8aa21f36\">\n",
       "   <rect x=\"56.50625\" y=\"7.2\" width=\"195.3\" height=\"138.6\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 350x250 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_epochs, lr = 500, 1\n",
    "d2l.train_ch8(net, train_iter, vocab, lr, num_epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382f1fed-9eb5-41dc-9b74-b60308311c45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
