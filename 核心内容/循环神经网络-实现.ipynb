{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dee30d9-c85c-47ee-bbb0-0076e5c86951",
   "metadata": {},
   "source": [
    "# 循环神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67ce0cc-bac2-4f12-a9bd-f1c17655cfab",
   "metadata": {},
   "source": [
    "\n",
    "\r\n",
    "### 1. 无隐状态的神经网络\r\n",
    "无隐状态的神经网络是指那些在处理输入时**不保留任何历史信息**的网络。这类网络通常是前馈神经网络（Feedforward Neural Networks），比如多层感知机（MLP）或卷积神经网络（CNN）。它们的输入和输出之间没有时间上的依赖关。\r\n",
    "\r\n",
    "#### 1.1 数学表示\r\n",
    "假设输入为 $\\mathbf{x}_t$，输出为 $\\mathbf{y}_t$，无隐状态的神经网络可以表示为：\r\n",
    "$$\r\n",
    "\\mathbf{y}_t = f(\\mathbf{x}_t; \\mathbf{W})\r\n",
    "$$\r\n",
    "其中：\r\n",
    "- $f$ 是一个非线性函数（如ReLU、Sigmoid等），\r\n",
    "- $\\mathbf{W}$ 是网的参数（权重和偏置）。\r\n",
    "\r\n",
    "#### 1.2 工作机制\r\n",
    "- 每个输入 $\\mathbf{x}_t$ 是独立的，网络不会记住之前处理过的输入。\r\n",
    "- 输出 $\\mathbf{y}_t$ 仅依赖于当前输入 $\\mathbf{x}_t$，与之前的输入 $\\mathbf{x}_{t-1}, \\mathbf{x}_{-2}, \\dots$ 无关。\r\n",
    "\r\n",
    "#### 1.3 优点\r\n",
    "- 简单且易于实现。\r\n",
    "- 适合处理静数据（如图像分类、静态特征提取等）。\r\n",
    "\r\n",
    "#### 1.4 缺点\r\n",
    "- 无法处理序列数据中的时间依赖性。\r\n",
    "- 对于时间列、文本、语音等数据，无法捕捉上下文信息。\r\n",
    "\r\n",
    "#### 1.5 适用场景\r\n",
    "- 图像分类（CNN）。\n",
    "- 静态数据的回归或分类任务（MLP）。\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 2. 有隐状态的循环神经网络（RNN）\r\n",
    "有隐状态的循环神经网络（RNN）通过引入**隐状态（Hidden State）**来捕捉序列数据中的时间依赖性。隐状态可以被看作是一个“记忆单元，它保存了之前时间步的信息，并在处理当前输入时使用这些信息。\r\n",
    "\r\n",
    "#### 2.1 数学表示\r\n",
    "假设在时间步 $t$：\r\n",
    "- 输入为 $\\mathbf{x}_t$，\r\n",
    "- 隐状态为 $\\mathbf{h}_t$，\r\n",
    "- 输出为 $\\mathbf{y}_t$。\r\n",
    "\r\n",
    "RNN的更新规则可以表示为：\r\n",
    "$$\r\n",
    "\\mathbf{h}_t = f(\\mathbf{x}_t, \\mathbf{h}_{t-1}; \\mathbf{W})\r\n",
    "$$\r\n",
    "其中：\r\n",
    "- $f$ 是一个非线性函数（如tanh、ReLU等），\r\n",
    "- $\\mathbf{W}$ 是网络的参数（权重和偏置），\r\n",
    "- $\\mathbf{h}_{t-1}$ 是上一个时间步的隐状态。\r\n",
    "\r\n",
    "输出 $\\mathbf{y}_t$ 通常由隐状态 $\\mathbf{h}_t$ 决定：\r\n",
    "$$\r\n",
    "\\mathbf{y}_t = g(\\mathbf{h}_t; \\mathbf{V})\r\n",
    "$$\r\n",
    "其中：\r",
    "- $g$ 是输出函数（如softmax、线性变换等），\r\n",
    "- $\\mathbf{V}$ 是输出层的参数。\r\n",
    "\r\n",
    "#### 2.2 工作机制\r\n",
    "- RNN通过隐状态 $\\mathbf{h}_t$ 保留了之前时间步的信息。\r\n",
    "- 在每个时间步 $t$，RNN接收当前输入 $\\mathbf{x}_t$ 和上一个隐状态 $\\mathbf{h}_{t-1}$，计算当前隐状态 $\\mathbf{h}_t$ 和输出 $\\mathbf{y}_t$。\r\n",
    "- 隐状态 $\\mathbf{h}_t$ 可以被看作是对过去有输入 $\\mathbf{x}_1, \\mathbf{x}_2, \\dots, \\mathbf{x}_t$ 的总结。\r\n",
    "\r\n",
    "#### 2.3 优点\n",
    "- 能够处理序列数据中的时间依赖性。\r\n",
    "- 适合处理变长序列（如文本、语音、时间序列等）。\r\n",
    "- 通过隐状态捕捉上下文信息。\r\n",
    "\r\n",
    "#### 2.4 缺点\r\n",
    "- 训练过程中容易\n",
    "出现梯消失或梯度爆炸问题。\r\n",
    "- 长期依赖问题：RNN难以捕捉远距离时间步之间的依赖关系（尽管LSTM和GRU等改进模型可以缓解这一问题）。\r\n",
    "\r\n",
    "#### .5 适用场景\r\n",
    "- 自然语言处理（如文本生成、机器翻译）。\r\n",
    "- 时间序列预测（如股票价格预测、天气预测）。\r\n",
    "- 语音识别和处理。\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 3. 无隐状态与有隐状态网络的对比\r\n",
    "\r\n",
    "| 特性                  | 无隐状态网络（MLP/CNN）          | 有隐状态网络（RNN）               |\r\n",
    "|-----------------------|----------------------------------|----------------------------------|\r\n",
    "| **输入输出关系**       | 每个输入独立处理                 | 输入和输出之间存在时间依赖性     |\r\n",
    "| **隐状态**            | 无隐状态                         | 有隐状态，保存历史信息           |\r\n",
    "| **时间依赖性**        | 无法捕捉时间依赖性               | 能够捕捉时间依赖性               |\r\n",
    "| **适用数据**          | 静态数据（如图像）               | 序列数据（如文本、语音、时间序列）|\r\n",
    "| **训练难度**          | 相对简单                         | 较复杂，能出现梯度消失/爆炸    |\r\n",
    " **典型应用**          | 图像分类、静态特征提取           | 机器翻译、语音识别、时间序列预测 |\r\n",
    "\r\n",
    "---\r\n",
    "\r\n",
    "### 4. 数学上的进一步分析\r\n",
    "\r\n",
    "#### 4.1 无隐状态网络的局限性\r\n",
    "无隐状态网络的输出仅依赖于当前输入：\r\n",
    "$$\r",
    "\\mathbf{y}_t = f(\\mathbf{x}_t; \\mathbf{W})\r\n",
    "$$\r\n",
    "这种形式无法建模序列数据中的时间依赖性。例如，在自然语言处理中，一个词的含义可能依赖于前面的词。\r\n",
    "\r\n",
    "#### 4.2 RNN的时间展开\r\n",
    "RNN可以通过时间展开（Unrolling）来表示。假设序列长度为 $T$，RNN可以展开为：\r\n",
    "$$\r\n",
    "\\mathbf{h}_t = f(\\mathbf{x}_t, \\mathbf{h}_{t-1}; \\mathbf{W}), \\quad t = 1, 2, \\dots, T\r\n",
    "$$\r\n",
    "输出为：\r",
    "$$\r\n",
    "\\mathbf{y}_t = g(\\mathbf{h}_t; \\mathbf{V}), \\quad t = 1, 2, \\dots, T\r\n",
    "$$\r\n",
    "通过这种展开，RNN可以显式地建模序列数据中\n",
    "的解决RNN的局限性，后续提出了LSTM（长短期记忆网络）和GRU（门控循环单元），它们通过引入门控机制更好地处理长期依赖问题。\r\n",
    "\r\n",
    "希望这个更详细的解释能帮助你更好地理解无隐状态网络和有隐状态循环神经网络的区别与联系！如果还有疑问，欢迎继续提问！解决RNN的局限性，后续提出了LSTM（长短期记忆网络）和GRU（门控循环单元），它们通过引入门控机制更好地处理长期依赖问题。\r\n",
    "\r\n",
    "希望这个更详细的解释能帮助你更好地理解无隐状态网络和有隐状态循环神经网络的区别与联系！如果还有疑问，欢迎继续提问！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08672047-8f91-4043-8845-07c3b6d3db39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c535c97b-5fd6-4869-9d1d-29c7e881dac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9035,  0.3974, -0.4007, -1.7206],\n",
       "        [-4.2657, -2.9477, -2.1996, -0.8024],\n",
       "        [-2.4657,  0.3214,  1.7296, -3.0419]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, W_xh = torch.normal(0, 1, (3, 1)), torch.normal(0, 1, (1, 4))\n",
    "H, W_hh = torch.normal(0, 1, (3, 4)), torch.normal(0, 1, (4, 4))\n",
    "torch.matmul(X, W_xh) + torch.matmul(H,W_hh)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2a9abd-775c-414a-969f-c19b4ee301fb",
   "metadata": {},
   "source": [
    "首先，我们定义矩阵X、W_xh、H和W_hh，它们的形状分别为(3,1)、(1,4)、(3,4)和(4,4)。分别将X乘以W_xh，将H乘以W_hh，然后将\n",
    "这两个乘法相加，我们得到一个形状为(3,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe14583-1116-416a-b8db-c7fc014839ac",
   "metadata": {},
   "source": [
    "现在，我们沿列（轴1）拼接矩阵X和H，沿行（轴0）拼接矩阵W_xh和W_hh。这两个拼接分别产生形状(3, 5)和\n",
    "形状(5, 4)的矩阵。再将这两个拼接的矩阵相乘，我们得到与上面相同形状(3, 4)的输出矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "334d2735-cf1a-49dd-a6d4-0064bbbb767d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9035,  0.3974, -0.4007, -1.7206],\n",
       "        [-4.2657, -2.9477, -2.1996, -0.8024],\n",
       "        [-2.4657,  0.3214,  1.7296, -3.0419]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.matmul(torch.cat((X, H), 1), torch.cat((W_xh, W_hh),0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a411cc2b-12c4-4a87-8a80-a8069fe4f5cc",
   "metadata": {},
   "source": [
    "## 基于循环神经网络的字符级语言模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43532dd-2488-453c-8176-209151a91922",
   "metadata": {},
   "source": [
    "基于循环神经网络（RNN）的**字符级语言模型**是一种用于生成或预测文本序列的模型。它的目标是根据前面的字符序列，预测下一个字符的概率分布。以下是其核心思想和数学表达：\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **问题定义**\n",
    "字符级语言模型的输入是一个字符序列 $ x_1, x_2, \\dots, x_t $，其中每个 $ x_i $ 是一个字符（通常用 one-hot 向量表示）。模型的目标是预测下一个字符 $ x_{t+1} $ 的概率分布。\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **RNN 的结构**\n",
    "RNN 通过隐状态 $ h_t $ 来捕捉序列中的依赖关系。其计算过程如下：\n",
    "\n",
    "1. **输入表示**：\n",
    "   - 每个字符 $ x_t $ 是一个 one-hot 向量，维度为词汇表大小 $ V $。\n",
    "   - 通过嵌入矩阵 $ E $ 将 one-hot 向量映射为稠密向量：\n",
    "     $$\n",
    "     e_t = E x_t\n",
    "     $$\n",
    "     其中 $ e_t \\in \\mathbb{R}^d $，$ d $ 是嵌入维度。\n",
    "\n",
    "2. **隐状态更新**：\n",
    "   - RNN 的隐状态 $ h_t $ 通过当前输入 $ e_t $ 和前一个隐状态 $ h_{t-1} $ 计算得到：\n",
    "     $$\n",
    "     h_t = \\tanh(W_h h_{t-1} + W_e e_t + b_h)\n",
    "     $$\n",
    "     其中：\n",
    "     - $ W_h $ 是隐状态到隐状态的权重矩阵。\n",
    "     - $ W_e $ 是输入到隐状态的权重矩阵。\n",
    "     - $ b_h $ 是偏置项。\n",
    "     - $ \\tanh $ 是激活函数。\n",
    "\n",
    "3. **输出概率分布**：\n",
    "   - 通过隐状态 $ h_t $ 计算下一个字符的概率分布：\n",
    "     $$\n",
    "     o_t = W_o h_t + b_o\n",
    "     $$\n",
    "     其中：\n",
    "     - $ W_o $ 是隐状态到输出的权重矩阵。\n",
    "     - $ b_o $ 是偏置项。\n",
    "   - 使用 softmax 函数将 $ o_t $ 转换为概率分布：\n",
    "     $$\n",
    "     P(x_{t+1} | x_1, x_2, \\dots, x_t) = \\text{softmax}(o_t)\n",
    "     $$\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **训练目标**\n",
    "模型的训练目标是最大化训练数据的对数似然，即最小化负对数似然损失（交叉熵损失）。对于序列 $ x_1, x_2, \\dots, x_T $，损失函数为：\n",
    "$$\n",
    "\\mathcal{L} = -\\sum_{t=1}^{T-1} \\log P(x_{t+1} | x_1, x_2, \\dots, x_t)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **生成文本**\n",
    "训练完成后，模型可以通过以下方式生成文本：\n",
    "1. 给定一个初始字符 $ x_1 $。\n",
    "2. 根据当前字符和隐状态，预测下一个字符的概率分布。\n",
    "3. 从概率分布中采样一个字符作为 $ x_{t+1} $。\n",
    "4. 将 $ x_{t+1} $ 作为输入，重复上述过程，直到生成所需长度的文本。\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **关键特点**\n",
    "- **字符级建模**：模型以字符为单位进行预测，适合生成任意文本（包括拼写错误、标点符号等）。\n",
    "- **序列依赖**：RNN 的隐状态 $ h_t $ 捕捉了序列中的长期依赖关系。\n",
    "- **灵活性**：可以处理任意长度的序列，但可能面临梯度消失或梯度爆炸问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a2a7c7-c9cc-4f99-844a-fc13dc06a04b",
   "metadata": {},
   "source": [
    "## 困惑度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc05be9f-d571-4393-a320-3041027ef143",
   "metadata": {},
   "source": [
    "困惑度（Perplexity）是评估语言模型性能的常用指标，主要用于衡量模型对测试数据的预测能力。以下是其关键点：\n",
    "\n",
    "### 定义\n",
    "困惑度是衡量语言模型在给定测试数据上预测下一个词的不确定性。困惑度越低，模型的预测能力越强。\n",
    "\n",
    "### 计算公式\n",
    "困惑度公式为：\n",
    "$$\n",
    "\\text{Perplexity} = 2^{-\\frac{1}{N} \\sum_{i=1}^{N} \\log_2 P(w_i | w_1, w_2, \\dots, w_{i-1})} \n",
    "$$\n",
    "其中：\n",
    "- $ N $ 是测试数据的总词数。\n",
    "- $ P(w_i | w_1, w_2, \\dots, w_{i-1}) $ 是模型预测当前词 $ w_i $ 的概率。\n",
    "\n",
    "### 解释\n",
    "- **低困惑度**：模型对数据预测更准确。\n",
    "- **高困惑度**：模型预测能力较差。\n",
    "\n",
    "### 应用\n",
    "1. **模型比较**：用于比较不同语言模型的性能。\n",
    "2. **调参**：帮助调整模型超参数。\n",
    "3. **过拟合检测**：训练集困惑度低而测试集困惑度高可能表明过拟合。\n",
    "\n",
    "### 示例\n",
    "假设测试数据有100个词，模型预测每个词的概率为0.01，则困惑度为：\n",
    "$$\n",
    "\\text{Perplexity} = 2^{-\\frac{1}{100} \\times 100 \\times \\log_2 0.01} = 100 \n",
    "$$\n",
    "\n",
    "### 优缺点\n",
    "**优点**：\n",
    "- 计算简单，易于理解。\n",
    "- 广泛应用于语言模型评估。\n",
    "\n",
    "**缺点**：\n",
    "- 无法捕捉语义和上下文连贯性。\n",
    "- 对数据分布敏感，可能无法全面反映模型性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78045e4-b206-4dd2-b882-73a1e91eb01f",
   "metadata": {},
   "source": [
    "# 循环神经网络的从零开始实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6affab1-1789-40fa-9455-a9069f74e247",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f012c258-b559-463b-8886-94f77a15a8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, num_steps = 32, 35\n",
    "train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d83aaaf-0fa0-4771-b342-03596163004b",
   "metadata": {},
   "source": [
    "## 独热编码"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78d1fda-471a-4734-9b5f-c96a69e0e4a5",
   "metadata": {},
   "source": [
    "回想一下，在train_iter中，每个词元都表示为一个数字索引，将这些索引直接输入神经网络可能会使学习变\n",
    "得困难。我们通常将每个词元表示为更具表现力的特征向量。最简单的表示称为独热编码（one‐hotencoding）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35871c83-47b3-4e91-894a-82575a3602e1",
   "metadata": {},
   "source": [
    "简言之，将每个索引映射为相互不同的单位向量：假设词表中不同词元的数目为N（即len(vocab)），词元索\n",
    "引的范围为0到N−1。如果词元的索引是整数i，那么我们将创建一个长度为N的全0向量，并将第i处的元素\n",
    "设置为1。此向量是原始词元的一个独热向量。索引为0和2的独热向量如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb3c51e4-8392-4ec1-bddb-09da150cb3a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.one_hot(torch.tensor([0, 2]), len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1c1c61-0b44-474d-9af4-02379c0ab1f2",
   "metadata": {},
   "source": [
    "我们每次采样的小批量数据形状是二维张量：（批量大小，时间步数）。one_hot函数将这样一个小批量数据转\n",
    "换成三维张量，张量的最后一个维度等于词表大小（len(vocab)）。我们经常转换输入的维度，以便获得形状\n",
    "为（时间步数，批量大小，词表大小）的输出。这将使我们能够更方便地通过最外层的维度，一步一步地更\n",
    "新小批量数据的隐状态。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20977ff7-aa59-4e8f-8c74-81e3d2f7aa9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 2, 28])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.arange(10).reshape((2, 5))\n",
    "F.one_hot(X.T, 28).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976af399-753f-4a39-a7cc-f3342bf72059",
   "metadata": {},
   "source": [
    "## 初始化模型参数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3c73ff-e045-49aa-8ca5-c233b1225451",
   "metadata": {},
   "source": [
    "隐藏单元数num_hiddens是一个可调的超参数。当训练语\n",
    "言模型时，输入和输出来自相同的词表。因此，它们具有相同的维度，即词表的大小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b13ecfdc-423a-48a4-a86a-b57b8f982333",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  # 导入PyTorch库\n",
    "\n",
    "def get_params(vocab_size, num_hiddens, device):\n",
    "    \"\"\"\n",
    "    初始化RNN模型的参数。\n",
    "\n",
    "    参数:\n",
    "    - vocab_size: 词汇表的大小，即输入和输出的维度。\n",
    "    - num_hiddens: 隐藏层的大小，即隐藏层的神经元数量。\n",
    "    - device: 计算设备（如CPU或GPU），用于指定参数存储的位置。\n",
    "\n",
    "    返回:\n",
    "    - params: 包含所有模型参数的列表，这些参数已经设置为需要计算梯度。\n",
    "    \"\"\"\n",
    "    \n",
    "    # 输入和输出的维度相同，因为这是一个简单的RNN模型\n",
    "    num_inputs = num_outputs = vocab_size\n",
    "\n",
    "    # 定义一个辅助函数，用于生成服从正态分布的随机数\n",
    "    def normal(shape):\n",
    "        \"\"\"\n",
    "        生成服从正态分布的随机数，并乘以0.01以缩小初始值的范围。\n",
    "\n",
    "        参数:\n",
    "        - shape: 张量的形状。\n",
    "\n",
    "        返回:\n",
    "        - 一个形状为`shape`的张量，其值服从正态分布，且乘以0.01。\n",
    "        \"\"\"\n",
    "        return torch.randn(size=shape, device=device) * 0.01\n",
    "\n",
    "    # 初始化隐藏层的参数\n",
    "    W_xh = normal((num_inputs, num_hiddens))  # 输入到隐藏层的权重矩阵\n",
    "    W_hh = normal((num_hiddens, num_hiddens))  # 隐藏层到隐藏层的权重矩阵\n",
    "    b_h = torch.zeros(num_hiddens, device=device)  # 隐藏层的偏置向量\n",
    "\n",
    "    # 初始化输出层的参数\n",
    "    W_hq = normal((num_hiddens, num_outputs))  # 隐藏层到输出层的权重矩阵\n",
    "    b_q = torch.zeros(num_outputs, device=device)  # 输出层的偏置向量\n",
    "\n",
    "    # 将所有参数放入一个列表中\n",
    "    params = [W_xh, W_hh, b_h, W_hq, b_q]\n",
    "\n",
    "    # 设置所有参数为需要计算梯度\n",
    "    for param in params:\n",
    "        param.requires_grad_(True)\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397776f3-0874-4e66-990a-73763d7f2c2d",
   "metadata": {},
   "source": [
    "## 循环神经网络模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f61b2b0-d357-459e-bef2-74382756edcc",
   "metadata": {},
   "source": [
    "为了定义循环神经网络模型，我们首先需要一个init_rnn_state函数在初始化时返回隐状态。这个函数的返\n",
    "回是一个张量，张量全用0填充，形状为（批量大小，隐藏单元数）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "46f890b9-96e5-46b6-a93f-adf2ea9d74a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  # 导入PyTorch库\n",
    "\n",
    "def init_rnn_state(batch_size, num_hiddens, device):\n",
    "    \"\"\"\n",
    "    初始化RNN的隐藏状态。\n",
    "\n",
    "    参数:\n",
    "    - batch_size: 批次大小，即一次输入的数据样本数量。\n",
    "    - num_hiddens: 隐藏层的大小，即隐藏层的神经元数量。\n",
    "    - device: 计算设备（如CPU或GPU），用于指定隐藏状态存储的位置。\n",
    "\n",
    "    返回:\n",
    "    - 一个包含隐藏状态的元组，隐藏状态是一个全零的张量。\n",
    "    \"\"\"\n",
    "    # 初始化隐藏状态为一个全零的张量\n",
    "    # 形状为 (batch_size, num_hiddens)，表示每个样本的隐藏状态\n",
    "    return (torch.zeros((batch_size, num_hiddens), device=device), )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6f73be-53ce-4842-a707-ffe8d7797dbf",
   "metadata": {},
   "source": [
    "再考虑定义一个RNN函数，定义如何在一个时间步内计算隐状态和输出。循环神经网络模型通过inputs最外层的维度\n",
    "实现循环，以便逐时间步更新小批量数据的隐状态H。此外，这里使用tanh函数作为激活函数。\n",
    "当元素在实数上满足均匀分布时，tanh函数的平均值为0。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1a93a9f9-96c9-4ec3-adfb-ade08a6ac973",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  # 导入PyTorch库\n",
    "\n",
    "def rnn(inputs, state, params):\n",
    "    \"\"\"\n",
    "    实现RNN的前向传播。\n",
    "\n",
    "    参数:\n",
    "    - inputs: 输入数据，形状为 (时间步数量, 批量大小, 词表大小)。\n",
    "    - state: 初始隐藏状态，是一个元组，包含一个形状为 (批量大小, 隐藏层大小) 的张量。\n",
    "    - params: 包含RNN模型参数的列表，顺序为 [W_xh, W_hh, b_h, W_hq, b_q]。\n",
    "\n",
    "    返回:\n",
    "    - outputs: 所有时间步的输出，形状为 (时间步数量 * 批量大小, 词表大小)。\n",
    "    - (H,): 更新后的隐藏状态，是一个元组，包含一个形状为 (批量大小, 隐藏层大小) 的张量。\n",
    "    \"\"\"\n",
    "    # 解包参数\n",
    "    W_xh, W_hh, b_h, W_hq, b_q = params\n",
    "\n",
    "    # 解包隐藏状态\n",
    "    H, = state\n",
    "\n",
    "    # 用于存储每个时间步的输出\n",
    "    outputs = []\n",
    "\n",
    "    # 遍历每个时间步的输入\n",
    "    # X的形状：(批量大小, 词表大小)\n",
    "    for X in inputs:\n",
    "        # 更新隐藏状态 H\n",
    "        # torch.mm(X, W_xh): 计算输入到隐藏层的线性变换\n",
    "        # torch.mm(H, W_hh): 计算隐藏层到隐藏层的线性变换\n",
    "        # b_h: 隐藏层的偏置\n",
    "        # torch.tanh: 应用tanh激活函数\n",
    "        H = torch.tanh(torch.mm(X, W_xh) + torch.mm(H, W_hh) + b_h)\n",
    "\n",
    "        # 计算当前时间步的输出 Y\n",
    "        # torch.mm(H, W_hq): 计算隐藏层到输出层的线性变换\n",
    "        # b_q: 输出层的偏置\n",
    "        Y = torch.mm(H, W_hq) + b_q\n",
    "\n",
    "        # 将当前时间步的输出 Y 添加到 outputs 列表中\n",
    "        outputs.append(Y)\n",
    "\n",
    "    # 将所有时间步的输出拼接成一个张量\n",
    "    # outputs 的形状：(时间步数量 * 批量大小, 词表大小)\n",
    "    outputs = torch.cat(outputs, dim=0)\n",
    "\n",
    "    # 返回输出和更新后的隐藏状态\n",
    "    return outputs, (H,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad95903f-73b6-46a8-9497-8df1f8854290",
   "metadata": {},
   "source": [
    "定义了所有需要的函数之后，接下来我们创建一个类来包装这些函数，并存储从零开始实现的循环神经网络\n",
    "模型的参数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "31c8f9d9-c0ab-434f-9cda-a3ed854a80cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModelScratch:  # @save\n",
    "    \"\"\"从零开始实现的循环神经网络模型\"\"\"\n",
    "    def __init__(self, vocab_size, num_hiddens, device, get_params, init_state, forward_fn):\n",
    "        \"\"\"\n",
    "        初始化RNN模型。\n",
    "\n",
    "        参数:\n",
    "        - vocab_size: 词汇表的大小。\n",
    "        - num_hiddens: 隐藏层的大小（隐藏单元的数量）。\n",
    "        - device: 计算设备（如CPU或GPU）。\n",
    "        - get_params: 用于初始化模型参数的函数。\n",
    "        - init_state: 用于初始化隐藏状态的函数。\n",
    "        - forward_fn: 用于实现前向传播的函数。\n",
    "        \"\"\"\n",
    "        self.vocab_size, self.num_hiddens = vocab_size, num_hiddens\n",
    "        self.params = get_params(vocab_size, num_hiddens, device)  # 初始化模型参数\n",
    "        self.init_state, self.forward_fn = init_state, forward_fn  # 初始化隐藏状态和前向传播函数\n",
    "\n",
    "    def __call__(self, X, state):\n",
    "        \"\"\"\n",
    "        定义模型的前向传播。\n",
    "\n",
    "        参数:\n",
    "        - X: 输入数据，形状为 (批量大小, 时间步数量)。\n",
    "        - state: 隐藏状态，形状为 (批量大小, 隐藏层大小)。\n",
    "\n",
    "        返回:\n",
    "        - 输出和新的隐藏状态。\n",
    "        \"\"\"\n",
    "        # 将输入X转换为one-hot编码，形状为 (时间步数量, 批量大小, 词表大小)\n",
    "        X = F.one_hot(X.T, self.vocab_size).type(torch.float32)\n",
    "        # 调用前向传播函数\n",
    "        return self.forward_fn(X, state, self.params)\n",
    "\n",
    "    def begin_state(self, batch_size, device):\n",
    "        \"\"\"\n",
    "        初始化隐藏状态。\n",
    "\n",
    "        参数:\n",
    "        - batch_size: 批量大小。\n",
    "        - device: 计算设备（如CPU或GPU）。\n",
    "\n",
    "        返回:\n",
    "        - 初始隐藏状态。\n",
    "        \"\"\"\n",
    "        return self.init_state(batch_size, self.num_hiddens, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162d9d90-f8c5-4c0a-a029-ba8a7ec5ddde",
   "metadata": {},
   "source": [
    "再进行一个模型实例化与前向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ed34d602-06e7-40f8-aa61-1034dbd9fa47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 2, 3, 4],\n",
       "        [5, 6, 7, 8, 9]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6f4e238e-af4b-4b47-96f6-4578d08c0165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([10, 28]), 1, torch.Size([2, 512]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 设置隐藏层大小\n",
    "num_hiddens = 512\n",
    "\n",
    "# 实例化RNN模型\n",
    "net = RNNModelScratch(len(vocab), num_hiddens, d2l.try_gpu(), get_params, init_rnn_state, rnn)\n",
    "\n",
    "# 初始化隐藏状态\n",
    "state = net.begin_state(X.shape[0], d2l.try_gpu())\n",
    "\n",
    "# 前向传播\n",
    "Y, new_state = net(X.to(d2l.try_gpu()), state)\n",
    "\n",
    "# 输出结果的形状\n",
    "Y.shape, len(new_state), new_state[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c04486-2e61-483b-a654-502d7ddd52d6",
   "metadata": {},
   "source": [
    "我们可以看到输出形状是（时间步数×批量大小，词表大小），而隐状态形状保持不变，即（批量大小，隐藏\n",
    "单元数）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a55bbb-67a4-4067-b3ee-3db8f2efc328",
   "metadata": {},
   "source": [
    "## 预测"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ea651a-e651-4fc6-b1cf-53d24a14adc1",
   "metadata": {},
   "source": [
    "先定义预测函数来生成prefix之后的新字符，其中的prefix是一个用户提供的包含多个字符的字符\n",
    "串。在循环遍历prefix中的开始字符时，我们不断地将隐状态传递到下一个时间步，但是不生成任何输出。这\n",
    "被称为预热（warm‐up）期，因为在此期间模型会自我更新（例如，更新隐状态），但不会进行预测。预热期\n",
    "结束后，隐状态的值通常比刚开始的初始值更适合预测，从而预测字符并输出它们。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0057c607-e2f8-46d2-b7ba-8549e9e6a2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ch8(prefix, num_preds, net, vocab, device): #@save\n",
    "    \"\"\"在prefix后面生成新字符\"\"\"\n",
    "    # 初始化RNN的隐藏状态\n",
    "    state = net.begin_state(batch_size=1, device=device)\n",
    "    \n",
    "    # 将前缀的第一个字符转换为索引，并存储在outputs列表中\n",
    "    outputs = [vocab[prefix[0]]]\n",
    "    \n",
    "    # 定义一个lambda函数，用于获取当前时间步的输入\n",
    "    get_input = lambda: torch.tensor([outputs[-1]], device=device).reshape((1, 1))\n",
    "    \n",
    "    # 预热期：处理前缀的剩余字符，更新RNN的隐藏状态\n",
    "    for y in prefix[1:]:  # 遍历前缀的剩余字符\n",
    "        _, state = net(get_input(), state)  # 更新隐藏状态\n",
    "        outputs.append(vocab[y])  # 将字符索引添加到outputs列表中\n",
    "    \n",
    "    # 预测num_preds步，生成新字符\n",
    "    for _ in range(num_preds):  # 循环num_preds次\n",
    "        y, state = net(get_input(), state)  # 预测下一个字符并更新隐藏状态\n",
    "        outputs.append(int(y.argmax(dim=1).reshape(1)))  # 将预测结果转换为索引并添加到outputs列表中\n",
    "    \n",
    "    # 将outputs列表中的索引转换为字符，并拼接成最终的字符串返回\n",
    "    return ''.join([vocab.idx_to_token[i] for i in outputs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae5c6de-4909-454f-8b62-6bbedce6732b",
   "metadata": {},
   "source": [
    "现在我们可以测试predict_ch8函数。我们将前缀指定为time traveller，并基于这个前缀生成10个后续字\n",
    "符。鉴于我们还没有训练网络，它会生成荒谬的预测结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fb2f6db1-a502-4538-97ca-179caa616409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'time traveller           '"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_ch8('time traveller ', 10, net, vocab, d2l.try_gpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46c4426-aabc-42dc-8a59-31699e1f4676",
   "metadata": {},
   "source": [
    "## 梯度裁剪"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ac43f8-b4ca-4aea-9308-303005e64844",
   "metadata": {},
   "source": [
    "**梯度裁剪（Gradient Clipping）**是一种在训练深度学习模型时常用的技术，主要用于解决梯度爆炸（Gradient Explosion）问题。梯度爆炸是指在训练过程中，梯度值变得非常大，导致模型参数更新幅度过大，从而破坏模型的稳定性，甚至导致训练失败。\n",
    "\n",
    "梯度裁剪通过限制梯度的大小，确保梯度值在一个合理的范围内，从而稳定训练过程。\n",
    "\n",
    "---\n",
    "\n",
    "### 梯度爆炸的原因\n",
    "在深度神经网络中，尤其是循环神经网络（RNN）或长序列模型中，梯度是通过反向传播算法计算的。由于反向传播是通过链式法则逐层计算梯度的，如果网络层数较深或某些层的权重较大，梯度可能会在反向传播过程中不断累积，导致梯度值变得非常大。\n",
    "\n",
    "梯度爆炸的表现：\n",
    "- 模型参数更新幅度过大。\n",
    "- 损失函数值突然变成 `NaN`（非数值）。\n",
    "- 训练过程不稳定。\n",
    "\n",
    "---\n",
    "\n",
    "### 梯度裁剪的原理\n",
    "梯度裁剪的核心思想是对梯度进行限制，使其不超过某个阈值。具体来说，梯度裁剪有两种常见的方式：\n",
    "\n",
    "1. **按值裁剪（Value Clipping）**：\n",
    "   - 直接对梯度中的每个元素进行裁剪，如果梯度的绝对值超过某个阈值，则将其截断到该阈值。\n",
    "   - 公式：\n",
    "     $$\n",
    "     g_{i} = \\begin{cases}\n",
    "     \\text{threshold} & \\text{if } g_{i} > \\text{threshold} \\\\\n",
    "     -\\text{threshold} & \\text{if } g_{i} < -\\text{threshold} \\\\\n",
    "     g_{i} & \\text{otherwise}\n",
    "     \\end{cases}\n",
    "     $$\n",
    "   - 其中，$ g_{i} $ 是梯度的第 $ i $ 个元素。\n",
    "\n",
    "2. **按范数裁剪（Norm Clipping）**：\n",
    "   - 计算梯度的范数（通常是 L2 范数），如果梯度的范数超过某个阈值，则将梯度按比例缩放，使其范数等于阈值。\n",
    "   - 公式：\n",
    "     $$\n",
    "     g = \\begin{cases}\n",
    "     g \\cdot \\frac{\\text{threshold}}{\\|g\\|} & \\text{if } \\|g\\| > \\text{threshold} \\\\\n",
    "     g & \\text{otherwise}\n",
    "     \\end{cases}\n",
    "     $$\n",
    "   - 其中，$ \\|g\\| $ 是梯度的 L2 范数。\n",
    "\n",
    "---\n",
    "\n",
    "### 梯度裁剪的作用\n",
    "1. **防止梯度爆炸**：\n",
    "   - 通过限制梯度的大小，避免模型参数更新幅度过大，从而稳定训练过程。\n",
    "\n",
    "2. **加速收敛**：\n",
    "   - 梯度裁剪可以使训练过程更加平滑，避免因梯度爆炸导致的震荡，从而加速模型的收敛。\n",
    "\n",
    "3. **提高模型稳定性**：\n",
    "   - 在训练深度模型（如 RNN、Transformer）时，梯度裁剪是一种常用的正则化技术，可以提高模型的稳定性。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0fae5532-13b6-4f82-87bd-ee72644334ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_clipping(net, theta): #@save\n",
    "    \"\"\"裁剪梯度\"\"\"\n",
    "    # 判断模型类型并获取可训练参数\n",
    "    if isinstance(net, nn.Module):\n",
    "        params = [p for p in net.parameters() if p.requires_grad]\n",
    "    else:\n",
    "        params = net.params\n",
    "    \n",
    "    # 计算梯度的L2范数\n",
    "    norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))\n",
    "    \n",
    "    # 如果梯度的范数超过阈值，则进行裁剪\n",
    "    if norm > theta:\n",
    "        for param in params:\n",
    "            param.grad[:] *= theta / norm  # 裁剪梯度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f79982-8547-4102-a249-090d1b3af3fe",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1a4488-f4af-456d-ab93-a01fb66b2677",
   "metadata": {},
   "source": [
    "在训练模型之前，让定义一个函数在一个迭代周期内训练模型。这里的三个特点是：\n",
    "\n",
    "1. 序列数据的不同采样方法（随机采样和顺序分区）将导致隐状态初始化的差异。\n",
    "2. 我们在更新模型参数之前裁剪梯度。这样的操作的目的是，即使训练过程中某个点上发生了梯度爆炸，也能保证模型不会发散。\n",
    "3. 我们用困惑度来评价模型。这样的度量确保了不同长度的序列具有可比性。\n",
    "\n",
    "具体来说，当使用顺序分区时，我们只在每个迭代周期的开始位置初始化隐状态。由于下一个小批量数据中\n",
    "的第i个子序列样本与当前第i个子序列样本相邻，因此当前小批量数据最后一个样本的隐状态，将用于初\r\n",
    "化下一个小批量数据第一个样本的隐状态。这样，存储在隐状态中的序列的历史信息可以在一个迭代周内\r\n",
    "流经相邻的子序列。然而，在任何一点隐状态的计算，都依赖于同一迭代周期中前面所有的小批量数，这\r\n",
    "使得梯度计算变得复杂。为了降低计算量，在处理任何一个小批量数据之前，我们先分离梯度，使隐状态\r\n",
    "的梯度计算总是限制在一个小批量数据的\n",
    "\n",
    "当使用随机抽样时，因为每个样本都是在一个随机位置抽样的，因此需要为每个迭代周期重新初始化隐状\n",
    "态。与3.6节中的train_epoch_ch3函数相同，updater是更新模型参数的常用函数。它既可以是从头开始\r\n",
    "现的d2l.sgd函数，也可以是深度学习框架中内置的优化函数。时间步内。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "22b2063b-7688-47e3-aedc-446fc83edef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch_ch8(net, train_iter, loss, updater, device, use_random_iter):\n",
    "    \"\"\"训练网络一个迭代周期（定义见第8章）\"\"\"\n",
    "    state, timer = None, d2l.Timer()  # 初始化隐藏状态和计时器\n",
    "    metric = d2l.Accumulator(2)  # 累积训练损失和词元数量\n",
    "\n",
    "    # 遍历训练数据\n",
    "    for X, Y in train_iter:\n",
    "        # 初始化隐藏状态\n",
    "        if state is None or use_random_iter:\n",
    "            state = net.begin_state(batch_size=X.shape[0], device=device)\n",
    "        else:\n",
    "            if isinstance(net, nn.Module) and not isinstance(state, tuple):\n",
    "                state.detach_()  # 对于nn.GRU，state是单个张量\n",
    "            else:\n",
    "                for s in state:\n",
    "                    s.detach_()  # 对于nn.LSTM或自定义模型，state是元组\n",
    "\n",
    "        # 准备数据\n",
    "        y = Y.T.reshape(-1)\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # 前向传播\n",
    "        y_hat, state = net(X, state)\n",
    "\n",
    "        # 计算损失\n",
    "        l = loss(y_hat, y.long()).mean()\n",
    "\n",
    "        # 反向传播和梯度裁剪\n",
    "        if isinstance(updater, torch.optim.Optimizer):\n",
    "            updater.zero_grad()\n",
    "            l.backward()\n",
    "            grad_clipping(net, 1)  # 裁剪梯度\n",
    "            updater.step()\n",
    "        else:\n",
    "            l.backward()\n",
    "            grad_clipping(net, 1)  # 裁剪梯度\n",
    "            updater(batch_size=1)  # 手动更新参数\n",
    "\n",
    "        # 累积损失和词元数量\n",
    "        metric.add(l * y.numel(), y.numel())\n",
    "\n",
    "    # 返回平均损失和每个词元的训练时间\n",
    "    return math.exp(metric[0] / metric[1]), metric[1] / timer.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcf6e62-0fea-4412-bcf4-52e003c185a1",
   "metadata": {},
   "source": [
    "循环神经网络模型的训练函数既支持从零开始实现，也可以使用高级API来实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c8751498-d3e5-40eb-847d-30ddb912301c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ch8(net, train_iter, vocab, lr, num_epochs, device, use_random_iter=False):\n",
    "    \"\"\"训练模型（定义见第8章）\"\"\"\n",
    "    loss = nn.CrossEntropyLoss()  # 初始化交叉熵损失函数\n",
    "    animator = d2l.Animator(xlabel='epoch', ylabel='perplexity', legend=['train'], xlim=[10, num_epochs])  # 初始化动画工具\n",
    "\n",
    "    # 初始化优化器\n",
    "    if isinstance(net, nn.Module):\n",
    "        updater = torch.optim.SGD(net.parameters(), lr)  # 使用 PyTorch 的 SGD 优化器\n",
    "    else:\n",
    "        updater = lambda batch_size: d2l.sgd(net.params, lr, batch_size)  # 使用自定义的梯度下降函数\n",
    "\n",
    "    # 定义预测函数\n",
    "    predict = lambda prefix: predict_ch8(prefix, 50, net, vocab, device)  # 定义预测函数\n",
    "\n",
    "    # 训练和预测\n",
    "    for epoch in range(num_epochs):\n",
    "        ppl, speed = train_epoch_ch8(net, train_iter, loss, updater, device, use_random_iter)  # 训练一个周期\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(predict('time traveller'))  # 每隔 10 个周期生成预测文本\n",
    "            animator.add(epoch + 1, [ppl])  # 将当前周期的困惑度添加到动画工具中\n",
    "\n",
    "    # 打印最终结果\n",
    "    print(f'困惑度 {ppl:.1f}, {speed:.1f} 词元/秒 {str(device)}')\n",
    "    print(predict('time traveller'))  # 生成最终的预测文本\n",
    "    print(predict('traveller'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c60a21-f70a-4509-9dd6-fb59aab52a92",
   "metadata": {},
   "source": [
    "现在，我们训练循环神经网络模型。因为我们在数据集中只使用了10000个词元，所以模型需要更多的迭代周期来更好地收敛。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e02bfd1a-03a2-4605-86c3-bf117e5adf26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "困惑度 1.1, 37416.3 词元/秒 cuda:0\n",
      "time traveller for so it will be convenient to speak of himwas e\n",
      "travelleryou can show black is white by argument said filby\n"
     ]
    }
   ],
   "source": [
    "num_epochs, lr = 500, 1\n",
    "train_ch8(net, train_iter, vocab, lr, num_epochs, d2l.try_gpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ab0319-f76e-4a8d-8164-74f281d67122",
   "metadata": {},
   "source": [
    "最后在检查一下使用随机抽样方法的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "82202e08-2e3c-44ad-a060-2a2c4444bbdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "困惑度 1.4, 36556.8 词元/秒 cuda:0\n",
      "time traveller proceeded anyreal body must have extension in fou\n",
      "traveller held in his hand was a glitteringmetallic framewo\n"
     ]
    }
   ],
   "source": [
    "net = RNNModelScratch(len(vocab), num_hiddens, d2l.try_gpu(), get_params,\n",
    "                     init_rnn_state, rnn)\n",
    "train_ch8(net, train_iter, vocab, lr, num_epochs, d2l.try_gpu(),\n",
    "             use_random_iter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054eac50-19e5-4a6a-ab0f-7ac56bfc8afb",
   "metadata": {},
   "source": [
    "# 循环神经网络的简洁实现"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aaccddc-53f8-4dec-80b7-a2945af65580",
   "metadata": {},
   "source": [
    "同样实现的是对该数据集的操作，不过这里使用的是深度学习框架中的高级API实现操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c2a5aa15-1135-4690-84f0-0f11cae2cd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from d2l import torch as d2l\n",
    "batch_size, num_steps = 32, 35\n",
    "train_iter, vocab = d2l.load_data_time_machine(batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447ba9a0-4012-4b6b-8474-d03dbd86afd2",
   "metadata": {},
   "source": [
    "## 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3d4e6960-59e0-48b7-bc00-297cc4ad9381",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hiddens = 256\n",
    "rnn_layer = nn.RNN(len(vocab), num_hiddens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e452d2a-dd90-4009-984f-5ac240acadfa",
   "metadata": {},
   "source": [
    "使用张量来初始化隐状态，它的形状是（隐藏层数，批量大小，隐藏单元数）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "784b543d-f509-4688-9446-4fa1024a7b2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 256])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = torch.zeros((1, batch_size, num_hiddens))\n",
    "state.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b51dc06-5a73-437f-b69b-3247ff9b810a",
   "metadata": {},
   "source": [
    "通过一个隐状态和一个输入，我们就可以用更新后的隐状态计算输出。需要强调的是，rnn_layer的“输出”（Y）不涉及输出层的计算：它是指每个时间步的隐状态，这些隐状态可以用作后续输出层的输入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "55413deb-98a4-4b1d-bddc-b981018b3d19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([35, 32, 256]), torch.Size([1, 32, 256]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.rand(size=(num_steps, batch_size, len(vocab)))\n",
    "Y, state_new = rnn_layer(X, state)\n",
    "Y.shape, state_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66680680-c14e-4563-8c23-a42beb45c387",
   "metadata": {},
   "source": [
    "为一个完整的循环神经网络模型定义了一个RNNModel类。注意，rnn_layer只包含隐藏的循环层，我们还需要创建一个单独的输出层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "099ba689-0bbd-4730-b620-699c81f7f5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    \"\"\"循环神经网络模型\"\"\"\n",
    "    def __init__(self, rnn_layer, vocab_size, **kwargs):\n",
    "        super(RNNModel, self).__init__(**kwargs)\n",
    "        self.rnn = rnn_layer  # RNN 层\n",
    "        self.vocab_size = vocab_size  # 词汇表大小\n",
    "        self.num_hiddens = self.rnn.hidden_size  # 隐藏单元数\n",
    "\n",
    "        # 检查 RNN 是否双向\n",
    "        if not self.rnn.bidirectional:\n",
    "            self.num_directions = 1  # 单向\n",
    "            self.linear = nn.Linear(self.num_hiddens, self.vocab_size)  # 全连接层\n",
    "        else:\n",
    "            self.num_directions = 2  # 双向\n",
    "            self.linear = nn.Linear(self.num_hiddens * 2, self.vocab_size)  # 全连接层\n",
    "\n",
    "    def forward(self, inputs, state):\n",
    "        # 将输入的词元索引转换为 one-hot 编码\n",
    "        X = F.one_hot(inputs.T.long(), self.vocab_size)\n",
    "        X = X.to(torch.float32)  # 转换为浮点型\n",
    "\n",
    "        # 将输入和初始状态传递给 RNN 层\n",
    "        Y, state = self.rnn(X, state)\n",
    "\n",
    "        # 将 RNN 的输出重塑为 (num_steps * batch_size, hidden_size)\n",
    "        output = self.linear(Y.reshape((-1, Y.shape[-1])))\n",
    "\n",
    "        return output, state\n",
    "\n",
    "    def begin_state(self, device, batch_size=1):\n",
    "        # 初始化隐藏状态\n",
    "        if not isinstance(self.rnn, nn.LSTM):\n",
    "            # nn.GRU 以张量作为隐状态\n",
    "            return torch.zeros((self.num_directions * self.rnn.num_layers,\n",
    "                                batch_size, self.num_hiddens),\n",
    "                               device=device)\n",
    "        else:\n",
    "            # nn.LSTM 以元组作为隐状态\n",
    "            return (torch.zeros((self.num_directions * self.rnn.num_layers,\n",
    "                                 batch_size, self.num_hiddens), device=device),\n",
    "                    torch.zeros((self.num_directions * self.rnn.num_layers,\n",
    "                                 batch_size, self.num_hiddens), device=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fb651a-6d50-4b9a-9801-8ba4c05ae38e",
   "metadata": {},
   "source": [
    "## 训练与预测"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31bf164-1519-4457-8b04-268b3a0d2c49",
   "metadata": {},
   "source": [
    "在训练模型之前，让我们基于一个具有随机权重的模型进行预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "347bd11b-33c4-44ee-82af-98b685b6971c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'time travellerelelelelel'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = d2l.try_gpu()\n",
    "net = RNNModel(rnn_layer, vocab_size=len(vocab))\n",
    "net = net.to(device)\n",
    "d2l.predict_ch8('time traveller', 10, net, vocab, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c1c125-1aab-4824-9ce8-c20e8e992fa8",
   "metadata": {},
   "source": [
    "很明显，这种模型根本不能输出好的结果。接下来，我们使用之前定义的超参数调用train_ch8，并且使\n",
    "用高级API训练模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1ffd8cc4-184d-4a21-a19f-db8d4e40a896",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "困惑度 1.3, 219164.2 词元/秒 cuda:0\n",
      "time travelleris fouptey folmer dal tas el shed are atway stin s\n",
      "travellerimit we and down thingedithas is alwags ceatevile \n"
     ]
    }
   ],
   "source": [
    "num_epochs, lr = 500, 1\n",
    "d2l.train_ch8(net, train_iter, vocab, lr, num_epochs, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114667de-dd45-4bc2-96ef-3aed260c20d6",
   "metadata": {},
   "source": [
    "# 通过时间反向传播（BPTT）\n",
    "\n",
    "通过时间反向传播（Backpropagation Through Time, BPTT）是训练循环神经网络（RNN）的核心算法。BPTT 是标准反向传播算法在时间序列数据上的扩展，它通过将 RNN 在时间轴上展开，形成一个前馈神经网络，然后在这个展开的网络上进行反向传播。\n",
    "\n",
    "## 1. 循环神经网络的梯度分析\n",
    "\n",
    "在 RNN 中，梯度计算涉及到时间步的展开。假设我们有一个 RNN，其隐藏状态 $h_t$ 和输出 $y_t$ 的计算公式如下：\n",
    "\n",
    "$$\n",
    "h_t = \\sigma(W_h h_{t-1} + W_x x_t + b_h)\n",
    "$$\n",
    "\n",
    "$$\n",
    "y_t = W_y h_t + b_y\n",
    "$$\n",
    "\n",
    "其中：\n",
    "- $h_t$ 是时间步 $t$ 的隐藏状态。\n",
    "- $x_t$ 是时间步 $t$ 的输入。\n",
    "- $W_h$ 是隐藏状态的权重矩阵。\n",
    "- $W_x$ 是输入的权重矩阵。\n",
    "- $b_h$ 是隐藏状态的偏置。\n",
    "- $W_y$ 是输出的权重矩阵。\n",
    "- $b_y$ 是输出的偏置。\n",
    "- $\\sigma$ 是激活函数（如 tanh 或 ReLU）。\n",
    "\n",
    "### 1.1 完全计算\n",
    "\n",
    "在完全计算中，我们计算所有时间步的梯度。假设我们有 $T$ 个时间步，损失函数 $L$ 是所有时间步损失的总和：\n",
    "\n",
    "$$\n",
    "L = \\sum_{t=1}^T L_t\n",
    "$$\n",
    "\n",
    "其中 $L_t$ 是时间步 $t$ 的损失。我们需要计算损失 $L$ 对参数 $\\theta$（如 $W_h$, $W_x$, $b_h$, $W_y$, $b_y$）的梯度。\n",
    "\n",
    "对于每个时间步 $t$，我们可以计算损失 $L_t$ 对隐藏状态 $h_t$ 的梯度 $\\frac{\\partial L_t}{\\partial h_t}$。然后，通过链式法则，我们可以计算 $\\frac{\\partial L}{\\partial \\theta}$：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\theta} = \\sum_{t=1}^T \\frac{\\partial L_t}{\\partial \\theta}\n",
    "$$\n",
    "\n",
    "具体来说，对于 $W_h$，我们有：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W_h} = \\sum_{t=1}^T \\frac{\\partial L_t}{\\partial h_t} \\frac{\\partial h_t}{\\partial W_h}\n",
    "$$\n",
    "\n",
    "其中 $\\frac{\\partial h_t}{\\partial W_h}$ 可以通过链式法则进一步展开。\n",
    "\n",
    "### 1.2 截断时间步\n",
    "\n",
    "完全计算在时间步 $T$ 较大时计算量非常大，因此可以采用截断时间步的方法。截断时间步的基本思想是只计算最近 $k$ 个时间步的梯度，忽略更早时间步的影响。这种方法称为截断 BPTT（Truncated BPTT）。\n",
    "\n",
    "在截断 BPTT 中，我们只计算从时间步 $t-k$ 到 $t$ 的梯度：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\theta} \\approx \\sum_{t=k}^T \\frac{\\partial L_t}{\\partial \\theta}\n",
    "$$\n",
    "\n",
    "这种方法减少了计算量，但可能会丢失一些长期依赖的信息。\n",
    "\n",
    "### 1.3 随机截断\n",
    "\n",
    "随机截断是另一种减少计算量的方法。与固定截断不同，随机截断在每个时间步随机选择一个截断点，只计算从该点到当前时间步的梯度。这种方法可以在一定程度上保持长期依赖的信息，同时减少计算量。\n",
    "\n",
    "### 1.4 比较策略\n",
    "\n",
    "完全计算、截断时间步和随机截断各有优缺点：\n",
    "- **完全计算**：计算量大，但保留了所有时间步的信息。\n",
    "- **截断时间步**：计算量小，但可能会丢失长期依赖。\n",
    "- **随机截断**：计算量适中，可以在一定程度上保持长期依赖。\n",
    "\n",
    "选择哪种策略取决于具体任务和计算资源的限制。\n",
    "\n",
    "## 2. 通过时间反向传播的细节\n",
    "\n",
    "在 BPTT 中，我们需要计算损失 $L$ 对每个时间步的隐藏状态 $h_t$ 的梯度 $\\frac{\\partial L}{\\partial h_t}$。由于 $h_t$ 依赖于 $h_{t-1}$，我们需要通过时间步反向传播梯度。\n",
    "\n",
    "### 2.1 反向传播过程\n",
    "\n",
    "假设我们有 $T$ 个时间步，损失函数 $L$ 是所有时间步损失的总和：\n",
    "\n",
    "$$\n",
    "L = \\sum_{t=1}^T L_t\n",
    "$$\n",
    "\n",
    "我们需要计算 $\\frac{\\partial L}{\\partial h_t}$。对于最后一个时间步 $T$，我们有：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial h_T} = \\frac{\\partial L_T}{\\partial h_T}\n",
    "$$\n",
    "\n",
    "对于其他时间步 $t < T$，我们有：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial h_t} = \\frac{\\partial L_t}{\\partial h_t} + \\frac{\\partial h_{t+1}}{\\partial h_t} \\frac{\\partial L}{\\partial h_{t+1}}\n",
    "$$\n",
    "\n",
    "其中 $\\frac{\\partial h_{t+1}}{\\partial h_t}$ 是隐藏状态 $h_{t+1}$ 对 $h_t$ 的梯度，可以通过链式法则计算。\n",
    "\n",
    "### 2.2 梯度消失和梯度爆炸\n",
    "\n",
    "在 RNN 中，梯度消失和梯度爆炸是常见的问题。由于梯度是通过时间步反向传播的，如果梯度在传播过程中不断缩小，就会导致梯度消失；如果梯度不断放大，就会导致梯度爆炸。\n",
    "\n",
    "梯度消失和梯度爆炸的原因在于 $\\frac{\\partial h_{t+1}}{\\partial h_t}$ 的大小。如果 $\\frac{\\partial h_{t+1}}{\\partial h_t}$ 的范数小于 1，梯度会逐渐消失；如果大于 1，梯度会逐渐爆炸。\n",
    "\n",
    "### 2.3 解决梯度消失和梯度爆炸的方法\n",
    "\n",
    "为了解决梯度消失和梯度爆炸问题，可以采用以下方法：\n",
    "- **梯度裁剪**：在梯度爆炸时，将梯度裁剪到一个合理的范围内。\n",
    "- **使用 LSTM 或 GRU**：LSTM 和 GRU 通过引入门控机制，能够更好地捕捉长期依赖，缓解梯度消失问题。\n",
    "- **初始化技巧**：通过合理的权重初始化，可以减少梯度消失和梯度爆炸的发生。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126d4759-cdcd-4299-8cc4-bc8dac435d98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
